{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('Training LUNG Cancer Dataset.csv', low_memory=True)\n",
    "# melanoma = pd.read_csv('Melanoma Validation Dataset.csv', low_memory=True)\n",
    "# lancent = pd.read_csv('Lancent_dataset_complete.txt', low_memory=True)\n",
    "extra_lung = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2099,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_sites = train.drop(['geo_accession', 'Labels'], axis=1)\n",
    "labels = train['Labels']\n",
    "\n",
    "# Functions for filling empty values \n",
    "def fill_missing_with_mean(row):\n",
    "    return row.fillna(row.mean())\n",
    "\n",
    "# Dataset conversion for validation\n",
    "lancent_test_standarized = scaler.fit_transform(lancent[rf_cpgs].fillna(0))\n",
    "lancent_y = lancent['Labels']\n",
    "melanoma_test_standarized = scaler.fit_transform(melanoma[rf_cpgs].fillna(0))\n",
    "melanoma_y = melanoma['Labels']\n",
    "\n",
    "# Functions for calling matrices \n",
    "def print_metrics(x, y):\n",
    "    print(\"Accuracy:\", accuracy_score(x, y))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV and ENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Dataset for analysis \n",
    "cpg_sites = train.drop(['geo_accession', 'Labels'], axis=1)\n",
    "labels = train['Labels']\n",
    "# Co Effeceint of Variation\n",
    "var = stats.variation(cpg_sites,axis=0)\n",
    "index1 = np.where(var > 1)\n",
    "np.save(\"CV\" + \".npy\", index1[0])\n",
    "res_list = [cpg_sites.columns[i] for i in index1[0].tolist()]\n",
    "# Elastic Net\n",
    "elanet_train = cpg_sites[res_list]\n",
    "enetCV = ElasticNetCV(alphas=[0.0001], l1_ratio=[.1], max_iter=10000).fit(elanet_train, labels)\n",
    "mask = enetCV.coef_ != 0\n",
    "index2 = np.where(mask == True)\n",
    "Enet_CV_CPGs = [cpg_sites.columns[i] for i in index2[0].tolist()]\n",
    "# Splitting based on CPG sites\n",
    "X = cpg_sites[Enet_CV_CPGs]\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"SGD Classifier\"\n",
    "Feature = \"CV + ENET\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGC Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_89 (Dense)            (None, 64)                21952     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30401 (118.75 KB)\n",
      "Trainable params: 30401 (118.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(64, kernel_initializer='normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.7))\n",
    "# Hidden layers\n",
    "# NN_model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# NN_model.add(Dropout(0.4))\n",
    "NN_model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mean_absolute_error', 'accuracy'])\n",
    "NN_model.summary()\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=128, epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train_standardized, y_train)\n",
    "model.score(X_test_standardized,y_test)\n",
    "feature_importances = pd.Series(model.feature_importances_, index=cpg_sites.columns)\n",
    "rf_cpgs = feature_importances[feature_importances > 0].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites[rf_cpgs]\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = type(model).__name__\n",
    "Feature = \"Random Forest\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGC Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(32, kernel_initializer='normal', input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "NN_model.add(Dropout(0.8))\n",
    "# Hidden layers\n",
    "NN_model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# NN_model.add(Dropout(0.8))\n",
    "# NN_model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer='ADAM', metrics=['mean_absolute_error', 'accuracy'])\n",
    "# NN_model.summary()\n",
    "\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=128, epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=RandomForestClassifier())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestClassifier())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier())"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "sel.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sel_features=sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.468345</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>0.873144</td>\n",
       "      <td>-0.514321</td>\n",
       "      <td>2.064817</td>\n",
       "      <td>-0.145675</td>\n",
       "      <td>0.217696</td>\n",
       "      <td>0.263477</td>\n",
       "      <td>0.331939</td>\n",
       "      <td>-1.403186</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044953</td>\n",
       "      <td>0.388860</td>\n",
       "      <td>-2.154744</td>\n",
       "      <td>1.166922</td>\n",
       "      <td>1.131037</td>\n",
       "      <td>-0.890519</td>\n",
       "      <td>1.529835</td>\n",
       "      <td>-1.232949</td>\n",
       "      <td>0.090685</td>\n",
       "      <td>0.055691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.066252</td>\n",
       "      <td>-0.186795</td>\n",
       "      <td>0.523093</td>\n",
       "      <td>-0.790456</td>\n",
       "      <td>1.589893</td>\n",
       "      <td>1.291559</td>\n",
       "      <td>0.269308</td>\n",
       "      <td>-0.500778</td>\n",
       "      <td>1.338485</td>\n",
       "      <td>-1.085779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736773</td>\n",
       "      <td>-0.319725</td>\n",
       "      <td>-1.236041</td>\n",
       "      <td>-1.023107</td>\n",
       "      <td>1.600488</td>\n",
       "      <td>0.044756</td>\n",
       "      <td>-0.760092</td>\n",
       "      <td>-0.238756</td>\n",
       "      <td>-0.299987</td>\n",
       "      <td>0.111686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046127</td>\n",
       "      <td>-0.320153</td>\n",
       "      <td>0.620740</td>\n",
       "      <td>0.420340</td>\n",
       "      <td>1.542819</td>\n",
       "      <td>0.682589</td>\n",
       "      <td>-0.731935</td>\n",
       "      <td>2.107823</td>\n",
       "      <td>0.191274</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536238</td>\n",
       "      <td>-0.097872</td>\n",
       "      <td>0.645706</td>\n",
       "      <td>-0.497251</td>\n",
       "      <td>-0.300688</td>\n",
       "      <td>-2.007196</td>\n",
       "      <td>0.283148</td>\n",
       "      <td>-0.248308</td>\n",
       "      <td>-0.229540</td>\n",
       "      <td>-0.453543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438166</td>\n",
       "      <td>-0.187506</td>\n",
       "      <td>0.272884</td>\n",
       "      <td>3.202991</td>\n",
       "      <td>-0.340843</td>\n",
       "      <td>-0.824004</td>\n",
       "      <td>0.361599</td>\n",
       "      <td>0.946656</td>\n",
       "      <td>0.994114</td>\n",
       "      <td>-1.435395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994723</td>\n",
       "      <td>-0.793743</td>\n",
       "      <td>0.624307</td>\n",
       "      <td>2.695938</td>\n",
       "      <td>0.683187</td>\n",
       "      <td>-0.927865</td>\n",
       "      <td>0.805871</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.953638</td>\n",
       "      <td>0.863869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.309239</td>\n",
       "      <td>-1.041539</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>-0.633602</td>\n",
       "      <td>-0.009333</td>\n",
       "      <td>0.702489</td>\n",
       "      <td>-0.993393</td>\n",
       "      <td>-0.349358</td>\n",
       "      <td>0.498109</td>\n",
       "      <td>-0.313008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>-0.317197</td>\n",
       "      <td>-0.651273</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.189521</td>\n",
       "      <td>-0.826415</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>0.392769</td>\n",
       "      <td>-0.268875</td>\n",
       "      <td>0.310314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.203557</td>\n",
       "      <td>-0.107976</td>\n",
       "      <td>-1.225613</td>\n",
       "      <td>-0.282706</td>\n",
       "      <td>-0.369113</td>\n",
       "      <td>0.371529</td>\n",
       "      <td>0.770072</td>\n",
       "      <td>-0.780172</td>\n",
       "      <td>0.087907</td>\n",
       "      <td>0.303605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127647</td>\n",
       "      <td>0.129925</td>\n",
       "      <td>0.684123</td>\n",
       "      <td>-1.003896</td>\n",
       "      <td>0.583457</td>\n",
       "      <td>0.935904</td>\n",
       "      <td>0.564009</td>\n",
       "      <td>0.285819</td>\n",
       "      <td>-1.098610</td>\n",
       "      <td>0.814702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.104622</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>-0.703987</td>\n",
       "      <td>0.374756</td>\n",
       "      <td>1.335409</td>\n",
       "      <td>-0.334483</td>\n",
       "      <td>-0.483122</td>\n",
       "      <td>-1.903233</td>\n",
       "      <td>0.794809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267187</td>\n",
       "      <td>-0.414590</td>\n",
       "      <td>0.771935</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>1.095927</td>\n",
       "      <td>0.113217</td>\n",
       "      <td>0.810632</td>\n",
       "      <td>0.651877</td>\n",
       "      <td>-0.531707</td>\n",
       "      <td>0.015332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.303608</td>\n",
       "      <td>-0.557164</td>\n",
       "      <td>0.600624</td>\n",
       "      <td>0.211757</td>\n",
       "      <td>0.546614</td>\n",
       "      <td>0.981578</td>\n",
       "      <td>-0.134954</td>\n",
       "      <td>-0.123749</td>\n",
       "      <td>0.440579</td>\n",
       "      <td>0.094138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762387</td>\n",
       "      <td>-0.432199</td>\n",
       "      <td>0.987322</td>\n",
       "      <td>-0.150390</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>0.184241</td>\n",
       "      <td>0.705365</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.377966</td>\n",
       "      <td>0.792129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.087017</td>\n",
       "      <td>0.234965</td>\n",
       "      <td>0.306975</td>\n",
       "      <td>-0.625321</td>\n",
       "      <td>-0.538577</td>\n",
       "      <td>0.779524</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>-0.086393</td>\n",
       "      <td>-0.689710</td>\n",
       "      <td>-0.731322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432347</td>\n",
       "      <td>-0.443068</td>\n",
       "      <td>1.021329</td>\n",
       "      <td>-0.537280</td>\n",
       "      <td>-0.354667</td>\n",
       "      <td>-1.177600</td>\n",
       "      <td>0.659029</td>\n",
       "      <td>1.004194</td>\n",
       "      <td>0.403447</td>\n",
       "      <td>1.367558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145521</td>\n",
       "      <td>0.152058</td>\n",
       "      <td>-0.459333</td>\n",
       "      <td>0.052201</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.556606</td>\n",
       "      <td>0.337298</td>\n",
       "      <td>-0.316794</td>\n",
       "      <td>-1.061417</td>\n",
       "      <td>0.544924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553787</td>\n",
       "      <td>-0.142016</td>\n",
       "      <td>0.914415</td>\n",
       "      <td>-1.252243</td>\n",
       "      <td>-0.082349</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190434</td>\n",
       "      <td>2.515544</td>\n",
       "      <td>0.575613</td>\n",
       "      <td>0.644237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.139193</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>0.529474</td>\n",
       "      <td>-0.023723</td>\n",
       "      <td>0.487396</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>-0.180642</td>\n",
       "      <td>-0.138206</td>\n",
       "      <td>-0.286442</td>\n",
       "      <td>-0.924335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.043515</td>\n",
       "      <td>-0.312510</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>1.940610</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>1.063312</td>\n",
       "      <td>0.099768</td>\n",
       "      <td>-1.214778</td>\n",
       "      <td>0.552570</td>\n",
       "      <td>-0.941951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.517745</td>\n",
       "      <td>0.940279</td>\n",
       "      <td>-0.481184</td>\n",
       "      <td>-0.352718</td>\n",
       "      <td>1.078213</td>\n",
       "      <td>-0.544736</td>\n",
       "      <td>-0.109968</td>\n",
       "      <td>-0.026651</td>\n",
       "      <td>0.319789</td>\n",
       "      <td>0.400743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.488562</td>\n",
       "      <td>0.325517</td>\n",
       "      <td>-2.137593</td>\n",
       "      <td>-0.104369</td>\n",
       "      <td>0.690180</td>\n",
       "      <td>-0.248975</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>-0.946281</td>\n",
       "      <td>0.196263</td>\n",
       "      <td>-0.082787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.119022</td>\n",
       "      <td>-1.461090</td>\n",
       "      <td>1.100497</td>\n",
       "      <td>-0.519058</td>\n",
       "      <td>0.465462</td>\n",
       "      <td>0.878980</td>\n",
       "      <td>0.082463</td>\n",
       "      <td>-0.358643</td>\n",
       "      <td>0.814442</td>\n",
       "      <td>-0.297973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455880</td>\n",
       "      <td>1.814259</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>1.398047</td>\n",
       "      <td>-0.738260</td>\n",
       "      <td>1.102037</td>\n",
       "      <td>0.677370</td>\n",
       "      <td>0.585452</td>\n",
       "      <td>0.117570</td>\n",
       "      <td>1.788051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.969224</td>\n",
       "      <td>-0.554200</td>\n",
       "      <td>-6.327866</td>\n",
       "      <td>-0.767350</td>\n",
       "      <td>-1.297014</td>\n",
       "      <td>-0.791628</td>\n",
       "      <td>-0.371570</td>\n",
       "      <td>-0.727755</td>\n",
       "      <td>1.080072</td>\n",
       "      <td>0.350536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719149</td>\n",
       "      <td>-0.580009</td>\n",
       "      <td>0.181391</td>\n",
       "      <td>-0.628812</td>\n",
       "      <td>-0.555926</td>\n",
       "      <td>-1.163196</td>\n",
       "      <td>-2.158674</td>\n",
       "      <td>1.046067</td>\n",
       "      <td>-0.340963</td>\n",
       "      <td>-2.843124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.608437</td>\n",
       "      <td>-0.209026</td>\n",
       "      <td>0.194252</td>\n",
       "      <td>-0.814380</td>\n",
       "      <td>-0.332841</td>\n",
       "      <td>-0.357825</td>\n",
       "      <td>-6.165703</td>\n",
       "      <td>-0.406818</td>\n",
       "      <td>-0.784704</td>\n",
       "      <td>-0.759348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850839</td>\n",
       "      <td>-0.996345</td>\n",
       "      <td>0.585316</td>\n",
       "      <td>-0.246422</td>\n",
       "      <td>-2.635975</td>\n",
       "      <td>1.109899</td>\n",
       "      <td>0.889646</td>\n",
       "      <td>2.381625</td>\n",
       "      <td>0.573255</td>\n",
       "      <td>1.313238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.326546</td>\n",
       "      <td>0.730518</td>\n",
       "      <td>-1.126157</td>\n",
       "      <td>-0.754688</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.562408</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.650994</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>-0.104322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064874</td>\n",
       "      <td>-0.466483</td>\n",
       "      <td>1.393496</td>\n",
       "      <td>-1.403715</td>\n",
       "      <td>1.379386</td>\n",
       "      <td>-0.109616</td>\n",
       "      <td>0.546066</td>\n",
       "      <td>-0.924909</td>\n",
       "      <td>-1.224564</td>\n",
       "      <td>0.073066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.359298</td>\n",
       "      <td>0.551866</td>\n",
       "      <td>0.666858</td>\n",
       "      <td>-0.544866</td>\n",
       "      <td>1.677323</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.063398</td>\n",
       "      <td>0.797150</td>\n",
       "      <td>0.317572</td>\n",
       "      <td>-0.424068</td>\n",
       "      <td>...</td>\n",
       "      <td>1.109273</td>\n",
       "      <td>1.453491</td>\n",
       "      <td>-0.139533</td>\n",
       "      <td>0.481557</td>\n",
       "      <td>-0.421575</td>\n",
       "      <td>-1.746287</td>\n",
       "      <td>1.480470</td>\n",
       "      <td>-0.866919</td>\n",
       "      <td>0.979884</td>\n",
       "      <td>0.559317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.884802</td>\n",
       "      <td>0.410563</td>\n",
       "      <td>0.512289</td>\n",
       "      <td>-0.017867</td>\n",
       "      <td>-0.284456</td>\n",
       "      <td>-3.030758</td>\n",
       "      <td>0.239595</td>\n",
       "      <td>-0.061990</td>\n",
       "      <td>1.114974</td>\n",
       "      <td>-0.047296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153761</td>\n",
       "      <td>-0.536793</td>\n",
       "      <td>0.950827</td>\n",
       "      <td>0.143435</td>\n",
       "      <td>-2.294624</td>\n",
       "      <td>1.196131</td>\n",
       "      <td>-2.372160</td>\n",
       "      <td>-0.999612</td>\n",
       "      <td>0.212737</td>\n",
       "      <td>-0.012930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.610742</td>\n",
       "      <td>0.279542</td>\n",
       "      <td>-0.545513</td>\n",
       "      <td>-0.275477</td>\n",
       "      <td>-0.337195</td>\n",
       "      <td>0.488568</td>\n",
       "      <td>0.192033</td>\n",
       "      <td>-0.088704</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.792507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722116</td>\n",
       "      <td>-0.289157</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>-0.144461</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>-0.463545</td>\n",
       "      <td>-1.058741</td>\n",
       "      <td>-0.341246</td>\n",
       "      <td>0.644786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.244206</td>\n",
       "      <td>0.965597</td>\n",
       "      <td>-0.806249</td>\n",
       "      <td>-0.467392</td>\n",
       "      <td>0.410591</td>\n",
       "      <td>-0.438830</td>\n",
       "      <td>-0.113727</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>-0.342394</td>\n",
       "      <td>1.248962</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.246516</td>\n",
       "      <td>-0.085461</td>\n",
       "      <td>-0.654631</td>\n",
       "      <td>-0.254895</td>\n",
       "      <td>0.144553</td>\n",
       "      <td>-0.068699</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>1.626570</td>\n",
       "      <td>2.237907</td>\n",
       "      <td>0.210993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.268000</td>\n",
       "      <td>-0.139907</td>\n",
       "      <td>-0.449395</td>\n",
       "      <td>-0.585887</td>\n",
       "      <td>0.595561</td>\n",
       "      <td>-0.876240</td>\n",
       "      <td>-0.543497</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>0.120593</td>\n",
       "      <td>0.211862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>-0.111716</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>0.541505</td>\n",
       "      <td>-0.253161</td>\n",
       "      <td>0.199152</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>-0.822217</td>\n",
       "      <td>1.425170</td>\n",
       "      <td>-0.012027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.824410</td>\n",
       "      <td>-0.397630</td>\n",
       "      <td>0.651541</td>\n",
       "      <td>-0.226962</td>\n",
       "      <td>-0.341528</td>\n",
       "      <td>0.312888</td>\n",
       "      <td>-0.158655</td>\n",
       "      <td>-0.576299</td>\n",
       "      <td>0.960908</td>\n",
       "      <td>-0.175099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195987</td>\n",
       "      <td>0.057815</td>\n",
       "      <td>0.558501</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>0.520552</td>\n",
       "      <td>1.142800</td>\n",
       "      <td>-1.983380</td>\n",
       "      <td>-0.450623</td>\n",
       "      <td>0.759296</td>\n",
       "      <td>0.839734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.763963</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.448084</td>\n",
       "      <td>-0.542762</td>\n",
       "      <td>0.153929</td>\n",
       "      <td>-0.781034</td>\n",
       "      <td>-0.493931</td>\n",
       "      <td>-0.268479</td>\n",
       "      <td>0.032037</td>\n",
       "      <td>1.328048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148145</td>\n",
       "      <td>-0.103433</td>\n",
       "      <td>-2.360376</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>-0.142327</td>\n",
       "      <td>-1.908463</td>\n",
       "      <td>0.693201</td>\n",
       "      <td>-0.325211</td>\n",
       "      <td>-0.887135</td>\n",
       "      <td>-1.654832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.284656</td>\n",
       "      <td>-0.231229</td>\n",
       "      <td>0.333175</td>\n",
       "      <td>-0.343496</td>\n",
       "      <td>0.220950</td>\n",
       "      <td>0.641742</td>\n",
       "      <td>1.710320</td>\n",
       "      <td>-0.159071</td>\n",
       "      <td>0.960967</td>\n",
       "      <td>-0.692926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685859</td>\n",
       "      <td>-0.508106</td>\n",
       "      <td>0.446362</td>\n",
       "      <td>-0.216957</td>\n",
       "      <td>-1.437294</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>-1.153901</td>\n",
       "      <td>1.079469</td>\n",
       "      <td>-0.241707</td>\n",
       "      <td>0.385623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.945257</td>\n",
       "      <td>-0.021135</td>\n",
       "      <td>-0.076473</td>\n",
       "      <td>-0.617481</td>\n",
       "      <td>0.076306</td>\n",
       "      <td>-0.931108</td>\n",
       "      <td>-0.382048</td>\n",
       "      <td>-0.408251</td>\n",
       "      <td>-2.565938</td>\n",
       "      <td>2.648592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.212009</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>0.852150</td>\n",
       "      <td>2.523780</td>\n",
       "      <td>-1.285238</td>\n",
       "      <td>-0.067270</td>\n",
       "      <td>-0.897504</td>\n",
       "      <td>2.443284</td>\n",
       "      <td>-0.171860</td>\n",
       "      <td>-2.278283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.854160</td>\n",
       "      <td>-0.298936</td>\n",
       "      <td>0.527060</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>-1.128391</td>\n",
       "      <td>0.218563</td>\n",
       "      <td>0.942686</td>\n",
       "      <td>-0.217358</td>\n",
       "      <td>-0.365779</td>\n",
       "      <td>-0.699775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527471</td>\n",
       "      <td>-0.502626</td>\n",
       "      <td>0.429862</td>\n",
       "      <td>0.832928</td>\n",
       "      <td>-1.205443</td>\n",
       "      <td>0.978404</td>\n",
       "      <td>-0.224551</td>\n",
       "      <td>-0.273562</td>\n",
       "      <td>0.486103</td>\n",
       "      <td>-1.848926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.137639</td>\n",
       "      <td>-1.192114</td>\n",
       "      <td>0.559532</td>\n",
       "      <td>3.000827</td>\n",
       "      <td>-1.874314</td>\n",
       "      <td>2.308138</td>\n",
       "      <td>1.131515</td>\n",
       "      <td>-0.413509</td>\n",
       "      <td>-2.556876</td>\n",
       "      <td>-0.416053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>-0.650087</td>\n",
       "      <td>1.258054</td>\n",
       "      <td>-0.364109</td>\n",
       "      <td>-0.726191</td>\n",
       "      <td>1.107486</td>\n",
       "      <td>-0.315785</td>\n",
       "      <td>-1.029691</td>\n",
       "      <td>-0.137388</td>\n",
       "      <td>1.385896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.921351</td>\n",
       "      <td>-0.424938</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>1.060758</td>\n",
       "      <td>-1.426968</td>\n",
       "      <td>0.369360</td>\n",
       "      <td>0.562767</td>\n",
       "      <td>-0.613913</td>\n",
       "      <td>0.993223</td>\n",
       "      <td>-0.777238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>3.042114</td>\n",
       "      <td>0.095016</td>\n",
       "      <td>0.499834</td>\n",
       "      <td>1.303926</td>\n",
       "      <td>0.995660</td>\n",
       "      <td>-2.144002</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.172931</td>\n",
       "      <td>-0.697276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.330254</td>\n",
       "      <td>-0.181926</td>\n",
       "      <td>-0.399608</td>\n",
       "      <td>0.133312</td>\n",
       "      <td>0.435666</td>\n",
       "      <td>0.703532</td>\n",
       "      <td>-1.235438</td>\n",
       "      <td>-0.307879</td>\n",
       "      <td>1.062676</td>\n",
       "      <td>-0.328550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.110122</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>-0.172285</td>\n",
       "      <td>0.188996</td>\n",
       "      <td>0.198070</td>\n",
       "      <td>-0.750083</td>\n",
       "      <td>-0.084595</td>\n",
       "      <td>0.410953</td>\n",
       "      <td>1.618637</td>\n",
       "      <td>-0.110634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.725665</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>-0.360019</td>\n",
       "      <td>-0.822667</td>\n",
       "      <td>0.101816</td>\n",
       "      <td>1.204092</td>\n",
       "      <td>-0.337925</td>\n",
       "      <td>-0.488256</td>\n",
       "      <td>1.164436</td>\n",
       "      <td>-0.974580</td>\n",
       "      <td>...</td>\n",
       "      <td>1.104412</td>\n",
       "      <td>-0.619594</td>\n",
       "      <td>0.543584</td>\n",
       "      <td>0.407094</td>\n",
       "      <td>0.611556</td>\n",
       "      <td>0.736876</td>\n",
       "      <td>1.531226</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.239290</td>\n",
       "      <td>0.768947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.200818</td>\n",
       "      <td>-0.542519</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.771435</td>\n",
       "      <td>-0.229037</td>\n",
       "      <td>1.543955</td>\n",
       "      <td>-0.210936</td>\n",
       "      <td>0.073468</td>\n",
       "      <td>0.899888</td>\n",
       "      <td>-0.710446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607129</td>\n",
       "      <td>1.318419</td>\n",
       "      <td>0.623185</td>\n",
       "      <td>-2.485316</td>\n",
       "      <td>1.620704</td>\n",
       "      <td>-0.283523</td>\n",
       "      <td>0.837234</td>\n",
       "      <td>-0.450286</td>\n",
       "      <td>-1.793893</td>\n",
       "      <td>0.026374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.045478</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.292694</td>\n",
       "      <td>-0.668272</td>\n",
       "      <td>-1.423425</td>\n",
       "      <td>-2.279506</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>-0.303814</td>\n",
       "      <td>-0.337137</td>\n",
       "      <td>0.862091</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387979</td>\n",
       "      <td>-0.428401</td>\n",
       "      <td>-0.390115</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>-0.185562</td>\n",
       "      <td>0.930015</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-1.076205</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>-0.319628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1.381248</td>\n",
       "      <td>0.626007</td>\n",
       "      <td>1.037963</td>\n",
       "      <td>-0.523144</td>\n",
       "      <td>-0.277041</td>\n",
       "      <td>-1.544855</td>\n",
       "      <td>-0.492396</td>\n",
       "      <td>0.397311</td>\n",
       "      <td>-2.452593</td>\n",
       "      <td>-1.026513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202611</td>\n",
       "      <td>4.654264</td>\n",
       "      <td>1.545132</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>-2.109601</td>\n",
       "      <td>1.261469</td>\n",
       "      <td>2.586153</td>\n",
       "      <td>-0.382131</td>\n",
       "      <td>1.362311</td>\n",
       "      <td>0.768245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.016018</td>\n",
       "      <td>-0.992527</td>\n",
       "      <td>0.529714</td>\n",
       "      <td>2.325461</td>\n",
       "      <td>1.949224</td>\n",
       "      <td>-0.671705</td>\n",
       "      <td>-0.931739</td>\n",
       "      <td>-0.322759</td>\n",
       "      <td>0.145803</td>\n",
       "      <td>2.465084</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.166989</td>\n",
       "      <td>2.676605</td>\n",
       "      <td>0.767184</td>\n",
       "      <td>1.047360</td>\n",
       "      <td>-0.327523</td>\n",
       "      <td>0.866535</td>\n",
       "      <td>-0.611067</td>\n",
       "      <td>0.709982</td>\n",
       "      <td>1.282063</td>\n",
       "      <td>-3.303747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.312821</td>\n",
       "      <td>1.317164</td>\n",
       "      <td>-0.830845</td>\n",
       "      <td>-0.685991</td>\n",
       "      <td>0.332855</td>\n",
       "      <td>0.380420</td>\n",
       "      <td>-0.237760</td>\n",
       "      <td>-0.285371</td>\n",
       "      <td>0.568799</td>\n",
       "      <td>-0.281416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>-0.114102</td>\n",
       "      <td>0.559787</td>\n",
       "      <td>-1.861512</td>\n",
       "      <td>0.844525</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.314849</td>\n",
       "      <td>0.271055</td>\n",
       "      <td>-0.532813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1.238341</td>\n",
       "      <td>-0.634568</td>\n",
       "      <td>0.548110</td>\n",
       "      <td>-0.186700</td>\n",
       "      <td>-0.711015</td>\n",
       "      <td>-0.760183</td>\n",
       "      <td>0.345533</td>\n",
       "      <td>-0.147953</td>\n",
       "      <td>-0.321544</td>\n",
       "      <td>2.275867</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.330954</td>\n",
       "      <td>-0.732387</td>\n",
       "      <td>0.416045</td>\n",
       "      <td>-0.225219</td>\n",
       "      <td>-1.768187</td>\n",
       "      <td>1.245896</td>\n",
       "      <td>-0.927493</td>\n",
       "      <td>-0.640630</td>\n",
       "      <td>1.812766</td>\n",
       "      <td>0.718375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.011058</td>\n",
       "      <td>-0.134012</td>\n",
       "      <td>-0.486240</td>\n",
       "      <td>-0.688488</td>\n",
       "      <td>-0.494558</td>\n",
       "      <td>-0.321116</td>\n",
       "      <td>0.276341</td>\n",
       "      <td>-0.404050</td>\n",
       "      <td>-0.559562</td>\n",
       "      <td>0.364398</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.044725</td>\n",
       "      <td>-0.471320</td>\n",
       "      <td>0.262435</td>\n",
       "      <td>0.359721</td>\n",
       "      <td>-1.396645</td>\n",
       "      <td>0.155956</td>\n",
       "      <td>-0.703042</td>\n",
       "      <td>0.760250</td>\n",
       "      <td>1.297756</td>\n",
       "      <td>-1.114401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.918739</td>\n",
       "      <td>5.967793</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>3.417922</td>\n",
       "      <td>-0.585390</td>\n",
       "      <td>-1.020096</td>\n",
       "      <td>-0.008235</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>1.085281</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484922</td>\n",
       "      <td>-0.138037</td>\n",
       "      <td>1.094639</td>\n",
       "      <td>-1.313783</td>\n",
       "      <td>-1.409975</td>\n",
       "      <td>0.958414</td>\n",
       "      <td>-1.225443</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>-0.759197</td>\n",
       "      <td>0.504428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.094814</td>\n",
       "      <td>-0.485621</td>\n",
       "      <td>-0.116393</td>\n",
       "      <td>-0.608102</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>0.544140</td>\n",
       "      <td>-0.043786</td>\n",
       "      <td>-0.407050</td>\n",
       "      <td>0.115272</td>\n",
       "      <td>-1.315706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535295</td>\n",
       "      <td>-0.116630</td>\n",
       "      <td>-1.491523</td>\n",
       "      <td>-0.912289</td>\n",
       "      <td>0.303727</td>\n",
       "      <td>-0.534743</td>\n",
       "      <td>-0.060109</td>\n",
       "      <td>-0.170359</td>\n",
       "      <td>-2.022562</td>\n",
       "      <td>-0.301322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.016467</td>\n",
       "      <td>-0.830169</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>1.671710</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>1.139880</td>\n",
       "      <td>-0.369356</td>\n",
       "      <td>-0.730226</td>\n",
       "      <td>2.178579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522683</td>\n",
       "      <td>-0.887820</td>\n",
       "      <td>1.081387</td>\n",
       "      <td>0.079484</td>\n",
       "      <td>0.466996</td>\n",
       "      <td>0.792390</td>\n",
       "      <td>-1.567516</td>\n",
       "      <td>1.905848</td>\n",
       "      <td>0.531075</td>\n",
       "      <td>0.928946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.723143</td>\n",
       "      <td>-0.555227</td>\n",
       "      <td>-0.569322</td>\n",
       "      <td>0.885521</td>\n",
       "      <td>-0.096578</td>\n",
       "      <td>0.525025</td>\n",
       "      <td>-0.245770</td>\n",
       "      <td>1.197542</td>\n",
       "      <td>-0.679343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709944</td>\n",
       "      <td>-0.408392</td>\n",
       "      <td>-0.217589</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.850215</td>\n",
       "      <td>-0.071967</td>\n",
       "      <td>-0.941385</td>\n",
       "      <td>-1.130990</td>\n",
       "      <td>-1.309602</td>\n",
       "      <td>0.059633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.919803</td>\n",
       "      <td>0.691707</td>\n",
       "      <td>0.940083</td>\n",
       "      <td>0.268925</td>\n",
       "      <td>2.335547</td>\n",
       "      <td>1.013785</td>\n",
       "      <td>-0.628996</td>\n",
       "      <td>4.668623</td>\n",
       "      <td>-0.289517</td>\n",
       "      <td>2.178436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.525210</td>\n",
       "      <td>1.831573</td>\n",
       "      <td>-0.251107</td>\n",
       "      <td>-0.749466</td>\n",
       "      <td>-0.044227</td>\n",
       "      <td>-2.218069</td>\n",
       "      <td>0.437456</td>\n",
       "      <td>1.170048</td>\n",
       "      <td>0.450166</td>\n",
       "      <td>0.980280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.383029</td>\n",
       "      <td>-0.749052</td>\n",
       "      <td>1.384740</td>\n",
       "      <td>1.426213</td>\n",
       "      <td>1.206688</td>\n",
       "      <td>1.090210</td>\n",
       "      <td>-1.100946</td>\n",
       "      <td>0.592848</td>\n",
       "      <td>-0.656407</td>\n",
       "      <td>1.266939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311397</td>\n",
       "      <td>-0.590479</td>\n",
       "      <td>-0.089206</td>\n",
       "      <td>1.032531</td>\n",
       "      <td>0.730341</td>\n",
       "      <td>-1.753375</td>\n",
       "      <td>1.135299</td>\n",
       "      <td>0.798803</td>\n",
       "      <td>1.164539</td>\n",
       "      <td>0.410870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.987682</td>\n",
       "      <td>0.789723</td>\n",
       "      <td>-0.725222</td>\n",
       "      <td>-0.104486</td>\n",
       "      <td>0.318614</td>\n",
       "      <td>-0.366373</td>\n",
       "      <td>-0.379533</td>\n",
       "      <td>-0.168513</td>\n",
       "      <td>0.297904</td>\n",
       "      <td>-0.347739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223146</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>-1.429544</td>\n",
       "      <td>0.272023</td>\n",
       "      <td>0.711725</td>\n",
       "      <td>-0.071517</td>\n",
       "      <td>0.211145</td>\n",
       "      <td>-0.074202</td>\n",
       "      <td>1.016763</td>\n",
       "      <td>-1.091769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.681699</td>\n",
       "      <td>-0.891222</td>\n",
       "      <td>0.075278</td>\n",
       "      <td>-0.518714</td>\n",
       "      <td>-0.492746</td>\n",
       "      <td>-2.073884</td>\n",
       "      <td>1.034398</td>\n",
       "      <td>-0.658565</td>\n",
       "      <td>-0.649638</td>\n",
       "      <td>-0.186893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676425</td>\n",
       "      <td>-0.530874</td>\n",
       "      <td>-1.273826</td>\n",
       "      <td>1.566829</td>\n",
       "      <td>0.654962</td>\n",
       "      <td>0.768243</td>\n",
       "      <td>0.087504</td>\n",
       "      <td>0.462239</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.574085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.844693</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>-0.746004</td>\n",
       "      <td>-0.290886</td>\n",
       "      <td>1.211992</td>\n",
       "      <td>0.381074</td>\n",
       "      <td>0.312299</td>\n",
       "      <td>-0.537850</td>\n",
       "      <td>0.709408</td>\n",
       "      <td>-0.622222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330613</td>\n",
       "      <td>-0.318799</td>\n",
       "      <td>0.512901</td>\n",
       "      <td>-0.749641</td>\n",
       "      <td>1.115714</td>\n",
       "      <td>0.521213</td>\n",
       "      <td>-1.115568</td>\n",
       "      <td>0.741069</td>\n",
       "      <td>-0.331798</td>\n",
       "      <td>0.229410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.062850</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>-0.716741</td>\n",
       "      <td>-0.723620</td>\n",
       "      <td>0.284323</td>\n",
       "      <td>0.386449</td>\n",
       "      <td>-0.637119</td>\n",
       "      <td>-0.195678</td>\n",
       "      <td>-0.846178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602512</td>\n",
       "      <td>-0.085883</td>\n",
       "      <td>0.292954</td>\n",
       "      <td>-0.258877</td>\n",
       "      <td>0.464914</td>\n",
       "      <td>0.689402</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>-0.373452</td>\n",
       "      <td>0.230564</td>\n",
       "      <td>1.122755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.834491</td>\n",
       "      <td>-0.699594</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>-0.199376</td>\n",
       "      <td>1.371056</td>\n",
       "      <td>0.723935</td>\n",
       "      <td>0.181465</td>\n",
       "      <td>1.200961</td>\n",
       "      <td>-0.202022</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057878</td>\n",
       "      <td>-0.264116</td>\n",
       "      <td>-1.168118</td>\n",
       "      <td>1.331192</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>-0.018619</td>\n",
       "      <td>-0.282975</td>\n",
       "      <td>-0.278644</td>\n",
       "      <td>-0.356848</td>\n",
       "      <td>-0.154458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.101958</td>\n",
       "      <td>-0.183704</td>\n",
       "      <td>0.303296</td>\n",
       "      <td>-0.318342</td>\n",
       "      <td>-0.673378</td>\n",
       "      <td>0.550684</td>\n",
       "      <td>0.358195</td>\n",
       "      <td>-0.552491</td>\n",
       "      <td>0.550992</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489434</td>\n",
       "      <td>-1.127910</td>\n",
       "      <td>0.984730</td>\n",
       "      <td>-0.433391</td>\n",
       "      <td>1.552780</td>\n",
       "      <td>0.746527</td>\n",
       "      <td>-0.798385</td>\n",
       "      <td>0.394120</td>\n",
       "      <td>-0.326934</td>\n",
       "      <td>0.924987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.430059</td>\n",
       "      <td>-0.175630</td>\n",
       "      <td>-0.401965</td>\n",
       "      <td>-0.753325</td>\n",
       "      <td>-0.372894</td>\n",
       "      <td>0.589955</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>-0.661840</td>\n",
       "      <td>1.556555</td>\n",
       "      <td>-2.132869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626780</td>\n",
       "      <td>0.352793</td>\n",
       "      <td>0.846648</td>\n",
       "      <td>-0.365493</td>\n",
       "      <td>1.357958</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>-0.070107</td>\n",
       "      <td>-0.340895</td>\n",
       "      <td>-2.261890</td>\n",
       "      <td>0.261115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.490711</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>0.245387</td>\n",
       "      <td>-0.779812</td>\n",
       "      <td>-1.375247</td>\n",
       "      <td>-2.209027</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>-0.379931</td>\n",
       "      <td>-1.288042</td>\n",
       "      <td>-0.035717</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.788851</td>\n",
       "      <td>-0.542848</td>\n",
       "      <td>-0.934400</td>\n",
       "      <td>-1.157434</td>\n",
       "      <td>-1.155087</td>\n",
       "      <td>0.386314</td>\n",
       "      <td>-0.286913</td>\n",
       "      <td>-1.110409</td>\n",
       "      <td>0.372993</td>\n",
       "      <td>-0.036534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.946898</td>\n",
       "      <td>-1.022953</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.429135</td>\n",
       "      <td>-0.026397</td>\n",
       "      <td>-0.917515</td>\n",
       "      <td>0.553416</td>\n",
       "      <td>-0.329108</td>\n",
       "      <td>-0.754083</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128578</td>\n",
       "      <td>-0.225194</td>\n",
       "      <td>-0.513659</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.721036</td>\n",
       "      <td>0.682624</td>\n",
       "      <td>1.126328</td>\n",
       "      <td>0.479468</td>\n",
       "      <td>-1.442462</td>\n",
       "      <td>0.278994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.498364</td>\n",
       "      <td>-0.737417</td>\n",
       "      <td>-0.411646</td>\n",
       "      <td>-0.552739</td>\n",
       "      <td>-0.754072</td>\n",
       "      <td>-1.463506</td>\n",
       "      <td>-0.724692</td>\n",
       "      <td>-0.584496</td>\n",
       "      <td>-2.506218</td>\n",
       "      <td>-0.559483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299637</td>\n",
       "      <td>0.343291</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>1.235381</td>\n",
       "      <td>-0.605020</td>\n",
       "      <td>-2.766362</td>\n",
       "      <td>-0.710676</td>\n",
       "      <td>0.480753</td>\n",
       "      <td>1.941014</td>\n",
       "      <td>-2.107872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.584919</td>\n",
       "      <td>-0.310870</td>\n",
       "      <td>-0.119476</td>\n",
       "      <td>1.322214</td>\n",
       "      <td>0.353631</td>\n",
       "      <td>-0.141365</td>\n",
       "      <td>0.987637</td>\n",
       "      <td>3.199988</td>\n",
       "      <td>0.520822</td>\n",
       "      <td>-1.091373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916403</td>\n",
       "      <td>-0.193330</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>-0.581729</td>\n",
       "      <td>0.682308</td>\n",
       "      <td>-1.642583</td>\n",
       "      <td>0.262188</td>\n",
       "      <td>-0.403934</td>\n",
       "      <td>1.294226</td>\n",
       "      <td>0.156651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.162879</td>\n",
       "      <td>0.020175</td>\n",
       "      <td>-0.158050</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>-0.134330</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>1.096313</td>\n",
       "      <td>-0.033198</td>\n",
       "      <td>0.053129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460332</td>\n",
       "      <td>-0.525820</td>\n",
       "      <td>-3.299305</td>\n",
       "      <td>0.736223</td>\n",
       "      <td>0.672375</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>0.538960</td>\n",
       "      <td>0.527446</td>\n",
       "      <td>-1.808974</td>\n",
       "      <td>-0.407521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.322335</td>\n",
       "      <td>1.658520</td>\n",
       "      <td>0.391860</td>\n",
       "      <td>1.381086</td>\n",
       "      <td>-0.744110</td>\n",
       "      <td>0.101312</td>\n",
       "      <td>-0.563813</td>\n",
       "      <td>-0.458070</td>\n",
       "      <td>-0.299595</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765881</td>\n",
       "      <td>-0.805719</td>\n",
       "      <td>-0.145340</td>\n",
       "      <td>-0.523912</td>\n",
       "      <td>-1.237360</td>\n",
       "      <td>0.153206</td>\n",
       "      <td>0.197136</td>\n",
       "      <td>0.763262</td>\n",
       "      <td>-0.491389</td>\n",
       "      <td>-0.816772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.267940</td>\n",
       "      <td>-0.273065</td>\n",
       "      <td>0.366455</td>\n",
       "      <td>-0.255555</td>\n",
       "      <td>-2.838450</td>\n",
       "      <td>0.160264</td>\n",
       "      <td>0.701836</td>\n",
       "      <td>-0.466672</td>\n",
       "      <td>0.937428</td>\n",
       "      <td>-0.696945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987230</td>\n",
       "      <td>-0.037183</td>\n",
       "      <td>-0.522682</td>\n",
       "      <td>0.274993</td>\n",
       "      <td>0.548657</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>-0.727874</td>\n",
       "      <td>0.522598</td>\n",
       "      <td>-0.109135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.325401</td>\n",
       "      <td>-0.330154</td>\n",
       "      <td>-0.234444</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>-1.073280</td>\n",
       "      <td>-0.758271</td>\n",
       "      <td>0.795963</td>\n",
       "      <td>2.984245</td>\n",
       "      <td>-1.415340</td>\n",
       "      <td>-0.134489</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.589478</td>\n",
       "      <td>-0.661429</td>\n",
       "      <td>0.301602</td>\n",
       "      <td>-2.048177</td>\n",
       "      <td>0.080675</td>\n",
       "      <td>-1.411915</td>\n",
       "      <td>-0.578436</td>\n",
       "      <td>-0.367693</td>\n",
       "      <td>-1.643935</td>\n",
       "      <td>0.280959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.088134</td>\n",
       "      <td>0.443159</td>\n",
       "      <td>-0.151061</td>\n",
       "      <td>-0.733901</td>\n",
       "      <td>-0.351973</td>\n",
       "      <td>-0.540345</td>\n",
       "      <td>0.457193</td>\n",
       "      <td>-0.425823</td>\n",
       "      <td>-0.221454</td>\n",
       "      <td>0.170288</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136238</td>\n",
       "      <td>-0.116378</td>\n",
       "      <td>-1.132216</td>\n",
       "      <td>-0.684635</td>\n",
       "      <td>-0.487742</td>\n",
       "      <td>-0.946363</td>\n",
       "      <td>0.246197</td>\n",
       "      <td>0.691111</td>\n",
       "      <td>-0.435208</td>\n",
       "      <td>-0.303709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-1.882262</td>\n",
       "      <td>-0.513184</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>-0.717102</td>\n",
       "      <td>-1.616959</td>\n",
       "      <td>0.820214</td>\n",
       "      <td>0.693190</td>\n",
       "      <td>-0.850433</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>-0.684563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148944</td>\n",
       "      <td>-0.559822</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>-0.434878</td>\n",
       "      <td>-0.549658</td>\n",
       "      <td>1.110178</td>\n",
       "      <td>2.432223</td>\n",
       "      <td>-0.520495</td>\n",
       "      <td>-0.723350</td>\n",
       "      <td>0.364719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   1.468345  0.961877  0.873144 -0.514321  2.064817 -0.145675  0.217696   \n",
       "1   2.066252 -0.186795  0.523093 -0.790456  1.589893  1.291559  0.269308   \n",
       "2  -0.046127 -0.320153  0.620740  0.420340  1.542819  0.682589 -0.731935   \n",
       "3  -1.438166 -0.187506  0.272884  3.202991 -0.340843 -0.824004  0.361599   \n",
       "4  -0.309239 -1.041539  0.008472 -0.633602 -0.009333  0.702489 -0.993393   \n",
       "5   0.203557 -0.107976 -1.225613 -0.282706 -0.369113  0.371529  0.770072   \n",
       "6  -1.104622  0.026245 -0.080639 -0.703987  0.374756  1.335409 -0.334483   \n",
       "7   0.303608 -0.557164  0.600624  0.211757  0.546614  0.981578 -0.134954   \n",
       "8  -0.087017  0.234965  0.306975 -0.625321 -0.538577  0.779524  0.394970   \n",
       "9   0.145521  0.152058 -0.459333  0.052201  0.738600  0.556606  0.337298   \n",
       "10  0.139193 -0.831213  0.529474 -0.023723  0.487396  0.585099 -0.180642   \n",
       "11  1.517745  0.940279 -0.481184 -0.352718  1.078213 -0.544736 -0.109968   \n",
       "12  2.119022 -1.461090  1.100497 -0.519058  0.465462  0.878980  0.082463   \n",
       "13 -1.969224 -0.554200 -6.327866 -0.767350 -1.297014 -0.791628 -0.371570   \n",
       "14 -1.608437 -0.209026  0.194252 -0.814380 -0.332841 -0.357825 -6.165703   \n",
       "15 -0.326546  0.730518 -1.126157 -0.754688  0.013822  0.562408 -0.041053   \n",
       "16  0.359298  0.551866  0.666858 -0.544866  1.677323  0.040003  0.063398   \n",
       "17  0.884802  0.410563  0.512289 -0.017867 -0.284456 -3.030758  0.239595   \n",
       "18 -0.610742  0.279542 -0.545513 -0.275477 -0.337195  0.488568  0.192033   \n",
       "19  0.244206  0.965597 -0.806249 -0.467392  0.410591 -0.438830 -0.113727   \n",
       "20 -0.268000 -0.139907 -0.449395 -0.585887  0.595561 -0.876240 -0.543497   \n",
       "21  1.824410 -0.397630  0.651541 -0.226962 -0.341528  0.312888 -0.158655   \n",
       "22  0.763963  0.192401  0.448084 -0.542762  0.153929 -0.781034 -0.493931   \n",
       "23  1.284656 -0.231229  0.333175 -0.343496  0.220950  0.641742  1.710320   \n",
       "24 -0.945257 -0.021135 -0.076473 -0.617481  0.076306 -0.931108 -0.382048   \n",
       "25 -0.854160 -0.298936  0.527060  0.007615 -1.128391  0.218563  0.942686   \n",
       "26  1.137639 -1.192114  0.559532  3.000827 -1.874314  2.308138  1.131515   \n",
       "27  1.921351 -0.424938  0.450495  1.060758 -1.426968  0.369360  0.562767   \n",
       "28 -0.330254 -0.181926 -0.399608  0.133312  0.435666  0.703532 -1.235438   \n",
       "29 -0.725665 -0.082335 -0.360019 -0.822667  0.101816  1.204092 -0.337925   \n",
       "30 -0.200818 -0.542519  0.622549  0.771435 -0.229037  1.543955 -0.210936   \n",
       "31 -1.045478 -0.267849 -0.292694 -0.668272 -1.423425 -2.279506  0.745146   \n",
       "32 -1.381248  0.626007  1.037963 -0.523144 -0.277041 -1.544855 -0.492396   \n",
       "33  0.016018 -0.992527  0.529714  2.325461  1.949224 -0.671705 -0.931739   \n",
       "34 -0.312821  1.317164 -0.830845 -0.685991  0.332855  0.380420 -0.237760   \n",
       "35 -1.238341 -0.634568  0.548110 -0.186700 -0.711015 -0.760183  0.345533   \n",
       "36  0.011058 -0.134012 -0.486240 -0.688488 -0.494558 -0.321116  0.276341   \n",
       "37  1.918739  5.967793  0.051552  3.417922 -0.585390 -1.020096 -0.008235   \n",
       "38 -0.094814 -0.485621 -0.116393 -0.608102  0.611756  0.544140 -0.043786   \n",
       "39 -1.016467 -0.830169  1.002107  1.671710  0.027076  0.675440  1.139880   \n",
       "40  0.437700  0.723143 -0.555227 -0.569322  0.885521 -0.096578  0.525025   \n",
       "41 -0.919803  0.691707  0.940083  0.268925  2.335547  1.013785 -0.628996   \n",
       "42 -0.383029 -0.749052  1.384740  1.426213  1.206688  1.090210 -1.100946   \n",
       "43  0.987682  0.789723 -0.725222 -0.104486  0.318614 -0.366373 -0.379533   \n",
       "44 -0.681699 -0.891222  0.075278 -0.518714 -0.492746 -2.073884  1.034398   \n",
       "45  0.844693  0.533396 -0.746004 -0.290886  1.211992  0.381074  0.312299   \n",
       "46  0.062850  0.004211  0.011833 -0.716741 -0.723620  0.284323  0.386449   \n",
       "47  0.834491 -0.699594  0.598131 -0.199376  1.371056  0.723935  0.181465   \n",
       "48 -1.101958 -0.183704  0.303296 -0.318342 -0.673378  0.550684  0.358195   \n",
       "49 -0.430059 -0.175630 -0.401965 -0.753325 -0.372894  0.589955  0.640618   \n",
       "50 -0.490711 -0.019985  0.245387 -0.779812 -1.375247 -2.209027  0.074822   \n",
       "51 -0.946898 -1.022953  0.051959  0.429135 -0.026397 -0.917515  0.553416   \n",
       "52  1.498364 -0.737417 -0.411646 -0.552739 -0.754072 -1.463506 -0.724692   \n",
       "53 -0.584919 -0.310870 -0.119476  1.322214  0.353631 -0.141365  0.987637   \n",
       "54  0.162879  0.020175 -0.158050  0.003217 -0.134330  0.011794  0.166630   \n",
       "55  0.322335  1.658520  0.391860  1.381086 -0.744110  0.101312 -0.563813   \n",
       "56  0.267940 -0.273065  0.366455 -0.255555 -2.838450  0.160264  0.701836   \n",
       "57 -0.325401 -0.330154 -0.234444 -0.008952 -1.073280 -0.758271  0.795963   \n",
       "58 -0.088134  0.443159 -0.151061 -0.733901 -0.351973 -0.540345  0.457193   \n",
       "59 -1.882262 -0.513184  0.227113 -0.717102 -1.616959  0.820214  0.693190   \n",
       "\n",
       "         7         8         9    ...       359       360       361       362  \\\n",
       "0   0.263477  0.331939 -1.403186  ...  1.044953  0.388860 -2.154744  1.166922   \n",
       "1  -0.500778  1.338485 -1.085779  ...  0.736773 -0.319725 -1.236041 -1.023107   \n",
       "2   2.107823  0.191274  0.902899  ...  0.536238 -0.097872  0.645706 -0.497251   \n",
       "3   0.946656  0.994114 -1.435395  ...  0.994723 -0.793743  0.624307  2.695938   \n",
       "4  -0.349358  0.498109 -0.313008  ...  0.480801 -0.317197 -0.651273  0.673200   \n",
       "5  -0.780172  0.087907  0.303605  ... -0.127647  0.129925  0.684123 -1.003896   \n",
       "6  -0.483122 -1.903233  0.794809  ... -0.267187 -0.414590  0.771935  0.012635   \n",
       "7  -0.123749  0.440579  0.094138  ...  0.762387 -0.432199  0.987322 -0.150390   \n",
       "8  -0.086393 -0.689710 -0.731322  ... -0.432347 -0.443068  1.021329 -0.537280   \n",
       "9  -0.316794 -1.061417  0.544924  ...  0.553787 -0.142016  0.914415 -1.252243   \n",
       "10 -0.138206 -0.286442 -0.924335  ...  1.043515 -0.312510  0.010342  1.940610   \n",
       "11 -0.026651  0.319789  0.400743  ... -0.488562  0.325517 -2.137593 -0.104369   \n",
       "12 -0.358643  0.814442 -0.297973  ...  0.455880  1.814259  0.184422  1.398047   \n",
       "13 -0.727755  1.080072  0.350536  ...  0.719149 -0.580009  0.181391 -0.628812   \n",
       "14 -0.406818 -0.784704 -0.759348  ...  0.850839 -0.996345  0.585316 -0.246422   \n",
       "15 -0.650994  0.989272 -0.104322  ... -0.064874 -0.466483  1.393496 -1.403715   \n",
       "16  0.797150  0.317572 -0.424068  ...  1.109273  1.453491 -0.139533  0.481557   \n",
       "17 -0.061990  1.114974 -0.047296  ...  0.153761 -0.536793  0.950827  0.143435   \n",
       "18 -0.088704  0.240939  0.792507  ... -0.722116 -0.289157 -1.022167 -0.144461   \n",
       "19  0.058733 -0.342394  1.248962  ... -1.246516 -0.085461 -0.654631 -0.254895   \n",
       "20  0.334983  0.120593  0.211862  ... -0.123005 -0.111716  0.955284  0.541505   \n",
       "21 -0.576299  0.960908 -0.175099  ...  0.195987  0.057815  0.558501  0.047898   \n",
       "22 -0.268479  0.032037  1.328048  ... -0.148145 -0.103433 -2.360376  0.961821   \n",
       "23 -0.159071  0.960967 -0.692926  ...  0.685859 -0.508106  0.446362 -0.216957   \n",
       "24 -0.408251 -2.565938  2.648592  ... -1.212009  0.013169  0.852150  2.523780   \n",
       "25 -0.217358 -0.365779 -0.699775  ...  0.527471 -0.502626  0.429862  0.832928   \n",
       "26 -0.413509 -2.556876 -0.416053  ...  0.071003 -0.650087  1.258054 -0.364109   \n",
       "27 -0.613913  0.993223 -0.777238  ...  0.087846  3.042114  0.095016  0.499834   \n",
       "28 -0.307879  1.062676 -0.328550  ...  1.110122  0.129252 -0.172285  0.188996   \n",
       "29 -0.488256  1.164436 -0.974580  ...  1.104412 -0.619594  0.543584  0.407094   \n",
       "30  0.073468  0.899888 -0.710446  ...  0.607129  1.318419  0.623185 -2.485316   \n",
       "31 -0.303814 -0.337137  0.862091  ... -1.387979 -0.428401 -0.390115  0.074712   \n",
       "32  0.397311 -2.452593 -1.026513  ...  1.202611  4.654264  1.545132  0.002539   \n",
       "33 -0.322759  0.145803  2.465084  ... -2.166989  2.676605  0.767184  1.047360   \n",
       "34 -0.285371  0.568799 -0.281416  ...  0.033636 -0.114102  0.559787 -1.861512   \n",
       "35 -0.147953 -0.321544  2.275867  ... -3.330954 -0.732387  0.416045 -0.225219   \n",
       "36 -0.404050 -0.559562  0.364398  ... -1.044725 -0.471320  0.262435  0.359721   \n",
       "37 -0.972401  1.085281  0.069042  ...  0.484922 -0.138037  1.094639 -1.313783   \n",
       "38 -0.407050  0.115272 -1.315706  ...  0.535295 -0.116630 -1.491523 -0.912289   \n",
       "39 -0.369356 -0.730226  2.178579  ... -0.522683 -0.887820  1.081387  0.079484   \n",
       "40 -0.245770  1.197542 -0.679343  ...  0.709944 -0.408392 -0.217589  0.001814   \n",
       "41  4.668623 -0.289517  2.178436  ... -0.525210  1.831573 -0.251107 -0.749466   \n",
       "42  0.592848 -0.656407  1.266939  ...  0.311397 -0.590479 -0.089206  1.032531   \n",
       "43 -0.168513  0.297904 -0.347739  ... -0.223146  0.074250 -1.429544  0.272023   \n",
       "44 -0.658565 -0.649638 -0.186893  ...  0.676425 -0.530874 -1.273826  1.566829   \n",
       "45 -0.537850  0.709408 -0.622222  ...  0.330613 -0.318799  0.512901 -0.749641   \n",
       "46 -0.637119 -0.195678 -0.846178  ...  0.602512 -0.085883  0.292954 -0.258877   \n",
       "47  1.200961 -0.202022  0.990105  ...  0.057878 -0.264116 -1.168118  1.331192   \n",
       "48 -0.552491  0.550992 -0.018437  ...  0.489434 -1.127910  0.984730 -0.433391   \n",
       "49 -0.661840  1.556555 -2.132869  ...  0.626780  0.352793  0.846648 -0.365493   \n",
       "50 -0.379931 -1.288042 -0.035717  ... -3.788851 -0.542848 -0.934400 -1.157434   \n",
       "51 -0.329108 -0.754083  0.226020  ...  0.128578 -0.225194 -0.513659  0.082654   \n",
       "52 -0.584496 -2.506218 -0.559483  ... -0.299637  0.343291 -0.000123  1.235381   \n",
       "53  3.199988  0.520822 -1.091373  ...  0.916403 -0.193330 -0.026129 -0.581729   \n",
       "54  1.096313 -0.033198  0.053129  ... -0.460332 -0.525820 -3.299305  0.736223   \n",
       "55 -0.458070 -0.299595  0.238980  ... -0.765881 -0.805719 -0.145340 -0.523912   \n",
       "56 -0.466672  0.937428 -0.696945  ...  0.987230 -0.037183 -0.522682  0.274993   \n",
       "57  2.984245 -1.415340 -0.134489  ... -1.589478 -0.661429  0.301602 -2.048177   \n",
       "58 -0.425823 -0.221454  0.170288  ... -1.136238 -0.116378 -1.132216 -0.684635   \n",
       "59 -0.850433  0.828745 -0.684563  ...  0.148944 -0.559822  0.031149 -0.434878   \n",
       "\n",
       "         363       364       365       366       367       368  \n",
       "0   1.131037 -0.890519  1.529835 -1.232949  0.090685  0.055691  \n",
       "1   1.600488  0.044756 -0.760092 -0.238756 -0.299987  0.111686  \n",
       "2  -0.300688 -2.007196  0.283148 -0.248308 -0.229540 -0.453543  \n",
       "3   0.683187 -0.927865  0.805871 -1.548329 -0.953638  0.863869  \n",
       "4   0.189521 -0.826415  0.103036  0.392769 -0.268875  0.310314  \n",
       "5   0.583457  0.935904  0.564009  0.285819 -1.098610  0.814702  \n",
       "6   1.095927  0.113217  0.810632  0.651877 -0.531707  0.015332  \n",
       "7   0.036079  0.184241  0.705365 -1.548329 -0.377966  0.792129  \n",
       "8  -0.354667 -1.177600  0.659029  1.004194  0.403447  1.367558  \n",
       "9  -0.082349  0.292043  0.190434  2.515544  0.575613  0.644237  \n",
       "10 -0.006238  1.063312  0.099768 -1.214778  0.552570 -0.941951  \n",
       "11  0.690180 -0.248975  0.475366 -0.946281  0.196263 -0.082787  \n",
       "12 -0.738260  1.102037  0.677370  0.585452  0.117570  1.788051  \n",
       "13 -0.555926 -1.163196 -2.158674  1.046067 -0.340963 -2.843124  \n",
       "14 -2.635975  1.109899  0.889646  2.381625  0.573255  1.313238  \n",
       "15  1.379386 -0.109616  0.546066 -0.924909 -1.224564  0.073066  \n",
       "16 -0.421575 -1.746287  1.480470 -0.866919  0.979884  0.559317  \n",
       "17 -2.294624  1.196131 -2.372160 -0.999612  0.212737 -0.012930  \n",
       "18  0.652486  0.031301 -0.463545 -1.058741 -0.341246  0.644786  \n",
       "19  0.144553 -0.068699 -0.169576  1.626570  2.237907  0.210993  \n",
       "20 -0.253161  0.199152  0.026295 -0.822217  1.425170 -0.012027  \n",
       "21  0.520552  1.142800 -1.983380 -0.450623  0.759296  0.839734  \n",
       "22 -0.142327 -1.908463  0.693201 -0.325211 -0.887135 -1.654832  \n",
       "23 -1.437294  0.864964 -1.153901  1.079469 -0.241707  0.385623  \n",
       "24 -1.285238 -0.067270 -0.897504  2.443284 -0.171860 -2.278283  \n",
       "25 -1.205443  0.978404 -0.224551 -0.273562  0.486103 -1.848926  \n",
       "26 -0.726191  1.107486 -0.315785 -1.029691 -0.137388  1.385896  \n",
       "27  1.303926  0.995660 -2.144002 -1.548329 -0.172931 -0.697276  \n",
       "28  0.198070 -0.750083 -0.084595  0.410953  1.618637 -0.110634  \n",
       "29  0.611556  0.736876  1.531226 -1.548329 -0.239290  0.768947  \n",
       "30  1.620704 -0.283523  0.837234 -0.450286 -1.793893  0.026374  \n",
       "31 -0.185562  0.930015  0.012071 -1.076205 -0.673049 -0.319628  \n",
       "32 -2.109601  1.261469  2.586153 -0.382131  1.362311  0.768245  \n",
       "33 -0.327523  0.866535 -0.611067  0.709982  1.282063 -3.303747  \n",
       "34  0.844525  0.036370  0.446342  0.314849  0.271055 -0.532813  \n",
       "35 -1.768187  1.245896 -0.927493 -0.640630  1.812766  0.718375  \n",
       "36 -1.396645  0.155956 -0.703042  0.760250  1.297756 -1.114401  \n",
       "37 -1.409975  0.958414 -1.225443  0.251200 -0.759197  0.504428  \n",
       "38  0.303727 -0.534743 -0.060109 -0.170359 -2.022562 -0.301322  \n",
       "39  0.466996  0.792390 -1.567516  1.905848  0.531075  0.928946  \n",
       "40  0.850215 -0.071967 -0.941385 -1.130990 -1.309602  0.059633  \n",
       "41 -0.044227 -2.218069  0.437456  1.170048  0.450166  0.980280  \n",
       "42  0.730341 -1.753375  1.135299  0.798803  1.164539  0.410870  \n",
       "43  0.711725 -0.071517  0.211145 -0.074202  1.016763 -1.091769  \n",
       "44  0.654962  0.768243  0.087504  0.462239  0.119469  0.574085  \n",
       "45  1.115714  0.521213 -1.115568  0.741069 -0.331798  0.229410  \n",
       "46  0.464914  0.689402  0.064300 -0.373452  0.230564  1.122755  \n",
       "47  0.958600 -0.018619 -0.282975 -0.278644 -0.356848 -0.154458  \n",
       "48  1.552780  0.746527 -0.798385  0.394120 -0.326934  0.924987  \n",
       "49  1.357958  0.098578 -0.070107 -0.340895 -2.261890  0.261115  \n",
       "50 -1.155087  0.386314 -0.286913 -1.110409  0.372993 -0.036534  \n",
       "51 -0.721036  0.682624  1.126328  0.479468 -1.442462  0.278994  \n",
       "52 -0.605020 -2.766362 -0.710676  0.480753  1.941014 -2.107872  \n",
       "53  0.682308 -1.642583  0.262188 -0.403934  1.294226  0.156651  \n",
       "54  0.672375  0.020585  0.538960  0.527446 -1.808974 -0.407521  \n",
       "55 -1.237360  0.153206  0.197136  0.763262 -0.491389 -0.816772  \n",
       "56  0.548657  0.089122 -0.084421 -0.727874  0.522598 -0.109135  \n",
       "57  0.080675 -1.411915 -0.578436 -0.367693 -1.643935  0.280959  \n",
       "58 -0.487742 -0.946363  0.246197  0.691111 -0.435208 -0.303709  \n",
       "59 -0.549658  1.110178  2.432223 -0.520495 -0.723350  0.364719  \n",
       "\n",
       "[60 rows x 369 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rf = np.delete(X, obj=~np.array(rf_sel_features), axis=1)\n",
    "pd.DataFrame(X_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling Analysis Using ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 866091)\n",
      "(60,)\n",
      "After OverSampling\n",
      "(90, 866091)\n",
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(cpg_sites)\n",
    "Y = labels\n",
    "#print(X)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print (\"After OverSampling\")\n",
    "oversampling = SMOTE()\n",
    "X,Y = oversampling.fit_resample(X,Y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 866091\n",
      "85% of total feature 736177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [475093] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features using ANOVA F-test: 134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "print(\"Total number of features:\", X.shape[1])\n",
    "a = round(0.85*(X.shape[1]))\n",
    "print(\"85% of total feature\", a)\n",
    "X_new = SelectKBest(f_classif, k=a).fit_transform(X, Y)\n",
    "#df=pd.DataFrame(X_new)\n",
    "fvals, pvals = f_classif(X_new, Y)\n",
    "to_remove = pvals >= (0.05/X_new.shape[1])\n",
    "X_anova = np.delete(X_new, obj=to_remove, axis=1)\n",
    "print(\"Number of selected features using ANOVA F-test:\", X_anova.shape[1])\n",
    "#print(X_anova)\n",
    "\n",
    "df1= cpg_sites_oversample\n",
    "print(\"Features using ANOVA\")\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=X_anova.shape[1])\n",
    "#df1 = pd.DataFrame(X)\n",
    "anova = bestfeatures.fit(df1, Y)\n",
    "dfscores = pd.DataFrame(anova.scores_)\n",
    "dfcolumns = pd.DataFrame(df1.columns)\n",
    "# concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['fFeature', 'fScore']  # naming the dataframe columns\n",
    "best_features = featureScores.nlargest(X_anova.shape[1], 'fScore')\n",
    "#print(best_features)\n",
    "anova_bestfeatures = best_features['fFeature'].tolist()\n",
    "print(len(anova_bestfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites_oversample[anova_bestfeatures]\n",
    "y = Y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "# X_train_standardized = scaler.fit_transform(X_train)\n",
    "# X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"SGD Classifier\"\n",
    "Feature = \"CV + ENET\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.1)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#  For loop using GRID CV (for cross validation)\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [50, 100, 200],\n",
    "    # 'max_depth': [None, 10, 20],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# rf_classifier = RandomForestClassifier()\n",
    "# \n",
    "# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "# \n",
    "#  For Loop in Models\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [i for i in range(1,50,5)],\n",
    "    # 'max_depth': [None, 10],\n",
    "    # 'min_samples_split': [2, 5],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': [ 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1],\n",
    "# }\n",
    "# best_accuracy = 0.0\n",
    "# best_params = None\n",
    "# import itertools\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "    # param_dict = dict(zip(param_grid.keys(), params))\n",
    "# \n",
    "    # rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "    # rf_classifier.fit(X_train, y_train)\n",
    "# \n",
    "    # y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "    # val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "# \n",
    "    # if val_accuracy > best_accuracy:\n",
    "        # best_accuracy = val_accuracy\n",
    "        # best_params = param_dict\n",
    "# \n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGD Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': [None,1,2,3,4,5, 10],\n",
    "#     'min_samples_split': [2, 3, 4, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4,5],\n",
    "#     'max_features': [None, 'sqrt', 'log2']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.8333333333333334\n",
      "Lancent :  0.6666666666666666\n",
      "Melanoma :  0.676923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(**best_params, random_state=20)\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Train : \" ,model.score(X_test,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 5],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'subsample': [0.8, 0.9, 1.0],\n",
    "#     'max_features': ['sqrt', 'log2', None]\n",
    "# }\n",
    "# best_accuracy = 0.0\n",
    "# best_params = None\n",
    "# import itertools\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "#     param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "#     rf_classifier = GradientBoostingClassifier(random_state=20, **param_dict)\n",
    "#     rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "#     val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    \n",
    "#     # val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "#     # val_accuracy = accuracy_score(lancent_y, val2_pred)\n",
    "\n",
    "#     # val_accuracy = (val_accuracy + val2_accuracy) / 2\n",
    "#     if val_accuracy > best_accuracy:\n",
    "#         best_accuracy = val_accuracy\n",
    "#         best_params = param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3993 - mean_absolute_error: 0.1774 - accuracy: 0.8333\n",
      "Test : [0.3992656171321869, 0.17739303410053253, 0.8333333134651184]\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.3871 - mean_absolute_error: 0.4425 - accuracy: 0.5926\n",
      "Lancent : [1.3871243000030518, 0.4424840807914734, 0.5925925970077515]\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.1508 - mean_absolute_error: 0.4644 - accuracy: 0.5538\n",
      "Melanome : [1.1508457660675049, 0.4643641412258148, 0.5538461804389954]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(128, kernel_initializer='normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.5))\n",
    "# Hidden layers\n",
    "NN_model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "NN_model.add(Dropout(0.8))\n",
    "\n",
    "NN_model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "NN_model.add(Dropout(0.9))\n",
    "\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['mean_absolute_error', 'accuracy'])\n",
    "# NN_model.summary()\n",
    "\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=64, validation_split=0.1 ,epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "MLP Model Test Accuracy: 0.8333333333333334\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "MLP Model Lancent Accuracy: 0.6790123456790124\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "MLP Model Melanoma Accuracy: 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Build the MLP model using Keras\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Dense(units=30, input_dim=X_train.shape[1], activation='linear'))\n",
    "\n",
    "model.add(Dropout(0.8))\n",
    "# Add hidden layers\n",
    "model.add(Dense(units=60, activation='relu'))\n",
    "\n",
    "# Add output layer (assuming binary classification)\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=0)\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"MLP Model Test Accuracy:\", accuracy)\n",
    "print(\"MLP Model Lancent Accuracy:\", accuracy_score(model.predict(((lancent_test_standarized)>0.5)).astype(int),lancent_y))\n",
    "print(\"MLP Model Melanoma Accuracy:\", accuracy_score(model.predict(((melanoma_test_standarized)>0.5)).astype(int),melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 1500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X,Y)\n",
    "dfscores = pd.DataFrame(RF.feature_importances_)\n",
    "dfcolumns = pd.DataFrame(cpg_sites.columns)\n",
    "fs = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "fs.columns = ['rfFeature', 'rfScore']\n",
    "rf_cpgs = fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False)['rfFeature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2096,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X)\n",
    "X_df.columns = cpg_sites.columns\n",
    "X_df = X_df[rf_cpgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Saved MOdel \n",
    "import joblib\n",
    "new_model = joblib.load('Saving Models/RF 70% on both.joblib')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2097,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.8333333333333334\n",
      "Lancent :  0.7283950617283951\n",
      "Melanoma :  0.7076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = new_model# RandomForestClassifier(**best_params, random_state=0)\n",
    "model.fit(X_train.values,y_train)\n",
    "print(\"Train : \" ,model.score(X_test,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterations for getting best random forest parameters selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2085,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [i for i in range(1,30)],\n",
    "    'max_depth': [None,2,3,5,8],\n",
    "    'min_samples_split': [2,3,4,5,6],\n",
    "    'min_samples_leaf': [ 1,2,3,4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'n_jobs' : [-1],\n",
    "}\n",
    "params_list = []\n",
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "import itertools\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "    rf_classifier.fit(X_train.values, y_train)\n",
    "\n",
    "    y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "    val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    \n",
    "    val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "    val2_accuracy = accuracy_score(lancent_y, val2_pred)\n",
    "    data_dict = {}\n",
    "    data_dict['Lancent'] = val2_accuracy\n",
    "    data_dict['Melanoma'] = val_accuracy\n",
    "    data_dict['Params'] = param_dict\n",
    "    params_list.append(data_dict)\n",
    "    # val_accuracy = (val_accuracy + val2_accuracy) / 2\n",
    "    if val_accuracy >= 0.7 and val2_accuracy >= 0.7:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_params = param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False).to_csv('RANDOM FOREST OVERSAMPLED CPGs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfFeature</th>\n",
       "      <th>rfScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>cg00031256</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>cg00035847</td>\n",
       "      <td>0.001243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>cg00049220</td>\n",
       "      <td>0.001125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>cg00064250</td>\n",
       "      <td>0.002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>cg00232731</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852571</th>\n",
       "      <td>cg27307781</td>\n",
       "      <td>0.001817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858795</th>\n",
       "      <td>cg27518051</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859820</th>\n",
       "      <td>cg27552081</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861169</th>\n",
       "      <td>cg27598086</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865431</th>\n",
       "      <td>ch.6.12702243F</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rfFeature   rfScore\n",
       "1193        cg00031256  0.000416\n",
       "1365        cg00035847  0.001243\n",
       "1858        cg00049220  0.001125\n",
       "2401        cg00064250  0.002363\n",
       "8302        cg00232731  0.003968\n",
       "...                ...       ...\n",
       "852571      cg27307781  0.001817\n",
       "858795      cg27518051  0.001947\n",
       "859820      cg27552081  0.000741\n",
       "861169      cg27598086  0.000414\n",
       "865431  ch.6.12702243F  0.002488\n",
       "\n",
       "[429 rows x 2 columns]"
      ]
     },
     "execution_count": 2113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df.sort_values(by='rfFeature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2093,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lancent</th>\n",
       "      <th>Melanoma</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': 5, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 5, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lancent  Melanoma                                             Params\n",
       "624  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "625  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "632  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "633  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "664  0.703704  0.569231  {'n_estimators': 6, 'max_depth': 5, 'min_sampl...\n",
       "..        ...       ...                                                ...\n",
       "315  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 5, 'min_sampl...\n",
       "346  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "347  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "354  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "355  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "\n",
       "[1680 rows x 3 columns]"
      ]
     },
     "execution_count": 2093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(by=['Lancent',\"Melanoma\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1868,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['CPGs'] = rf_cpgs\n",
    "df['Score'] = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1874,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_extra_CPGS = df[df['Score'] != 0]['CPGs'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1880,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = X_df[rf_extra_CPGS]\n",
    "Y_rf = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1881,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_rf, Y_rf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ABCMeta object argument after ** must be a mapping, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sbl/Documents/Cancer Methylation/new analysis/new.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_params, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train\u001b[39m.\u001b[39mvalues,y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain : \u001b[39m\u001b[39m\"\u001b[39m ,model\u001b[39m.\u001b[39mscore(X_test\u001b[39m.\u001b[39mvalues,y_test))\n",
      "\u001b[0;31mTypeError\u001b[0m: ABCMeta object argument after ** must be a mapping, not NoneType"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(**best_params, random_state=42)\n",
    "model.fit(X_train.values,y_train)\n",
    "print(\"Train : \" ,model.score(X_test.values,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for params in itertools.product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=0, **param_dict)\n",
    "    rf_classifier.fit(X_train.values, y_train)\n",
    "\n",
    "    y_val_pred1 = rf_classifier.predict(melanoma_test_standarized)\n",
    "    val1_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    y_val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "    val2_accuracy = accuracy_score(lancent_y, y_val2_pred)\n",
    "    print(val1_accuracy,val2_accuracy)\n",
    "#   val1_accuracy = 60\n",
    "#   val2_accuracy = 70\n",
    "    if val1_accuracy > 0.7 and val2_accuracy > 0.7:\n",
    "        data_dict = {}\n",
    "        data_dict['Lancent'] = val2_accuracy\n",
    "        data_dict['Melanoma'] =  val1_accuracy\n",
    "        data_dict['Params'] = param_dict\n",
    "        params_list.append(data_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = joblib.load('./Saving Models/RF 70% on both.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'float' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sbl/Documents/Cancer Methylation/new analysis/new.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val1_accuracy \u001b[39m=\u001b[39m \u001b[39m0.6615384615384615\u001b[39m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m val2_accuracy \u001b[39m=\u001b[39m \u001b[39m0.5555555555555556\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m val1_accuracy \u001b[39m&\u001b[39;49m val2_accuracy \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m70\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data_dict[\u001b[39m'\u001b[39m\u001b[39mLancent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m val1_accuracy\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'float' and 'float'"
     ]
    }
   ],
   "source": [
    "params_list = []\n",
    "val1_accuracy = 0.6615384615384615 \n",
    "val2_accuracy = 0.5555555555555556\n",
    "if val1_accuracy & val2_accuracy >= 70:\n",
    "    data_dict = {}\n",
    "    data_dict['Lancent'] = val1_accuracy\n",
    "    data_dict['Melanoma'] =  val2_accuracy\n",
    "    data_dict['Params'] = param_dict\n",
    "    params_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = pd.DataFrame()\n",
    "# label['labels'] = labels #.value_counts().plot(kind ='bar')\n",
    "# label['labels'].astype(str).replace('1',\"Responsive\").replace('0','Non Responsive').value_counts().plot(kind='bar', color='darkorange', title=\"No of Labels before OverSampling (original)\")\n",
    "# import matplotlib.pyplot as plt \n",
    "# plt.scatter(cpg_sites.mean(axis=1), cpg_sites.median(axis=1), c=la)\n",
    "# import seaborn as sns\n",
    "# sns.lmplot(x= 'Labels', y= 'cg00000029',data= train, hue='Labels', fit_reg=False)\n",
    "# fig = plt.gcf()\n",
    "# # set_size_inches(15, 10)\n",
    "# plt.show()\n",
    "# Model = \"SGD Classifier\"\n",
    "# Feature = \"CV + ENET\"\n",
    "# def print_metrics(x, y):\n",
    "#     print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "#     print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "#     print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # # Linear Regression \n",
    "# # linear_model = LinearRegression()\n",
    "# # linear_model.fit(X_train_standardized, y_train)\n",
    "# # y_pred = linear_model.predict(X_test_standardized)\n",
    "# # classification_report(y_pred, y_test)\n",
    "# # # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "# #                                                             \n",
    "# #                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# # model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.1)\n",
    "# # model.fit(X_train_standardized,y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "# #                                                             \n",
    "# #                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# # model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# # model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# # model.fit(X_train_standardized,y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "# #  For loop using GRID CV (for cross validation)\n",
    "# # param_grid = {\n",
    "#     # 'n_estimators': [50, 100, 200],\n",
    "#     # 'max_depth': [None, 10, 20],\n",
    "#     # 'min_samples_split': [2, 5, 10],\n",
    "#     # 'min_samples_leaf': [1, 2, 4],\n",
    "#     # 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#     # 'n_jobs' : [-1]\n",
    "# # }\n",
    "# # from sklearn.model_selection import GridSearchCV\n",
    "# # rf_classifier = RandomForestClassifier()\n",
    "# # \n",
    "# # grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "# # grid_search.fit(X_train, y_train)\n",
    "# # \n",
    "# # print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# # print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "# # \n",
    "# #  For Loop in Models\n",
    "# # param_grid = {\n",
    "#     # 'n_estimators': [i for i in range(1,50,5)],\n",
    "#     # 'max_depth': [None, 10],\n",
    "#     # 'min_samples_split': [2, 5],\n",
    "#     # 'min_samples_leaf': [1, 2, 4],\n",
    "#     # 'max_features': [ 'sqrt', 'log2'],\n",
    "#     # 'n_jobs' : [-1],\n",
    "# # }\n",
    "# # best_accuracy = 0.0\n",
    "# # best_params = None\n",
    "# # import itertools\n",
    "# # for params in itertools.product(*param_grid.values()):\n",
    "#     # param_dict = dict(zip(param_grid.keys(), params))\n",
    "# # \n",
    "#     # rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "#     # rf_classifier.fit(X_train, y_train)\n",
    "# # \n",
    "#     # y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "#     # val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "# # \n",
    "#     # if val_accuracy > best_accuracy:\n",
    "#         # best_accuracy = val_accuracy\n",
    "#         # best_params = param_dict\n",
    "# # \n",
    "\n",
    "# #                                                             \n",
    "# #                                         -------------------------- SGD Classifier\n",
    "\n",
    "# # model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# # model.fit(X_train_standardized, y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "# #                                                             \n",
    "# #                                                --------------------- SVM Classifier \n",
    "\n",
    "# # model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# # model.fit(X_train_standardized, y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "# #                                               ------------------------- Gradient Boost\n",
    "\n",
    "# # params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# # model = GradientBoostingClassifier(**params)\n",
    "# # model.fit(X_train_standardized, y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "# #                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# # model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# # model.fit(X_train_standardized,y_train)\n",
    "# # y_pred = model.predict(X_test_standardized)\n",
    "# # print_metrics(y_pred,y_test)\n",
    "# # y_pred = model.predict(lancent_test_standarized)\n",
    "# # print_metrics(y_pred,lancent_y)\n",
    "# # y_pred = model.predict(melanoma_test_standarized)\n",
    "# # print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# # model.fit(X_train_standardized,y_train)\n",
    "# # print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# # print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# # print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))\n",
    "# # param_grid = {\n",
    "# #     'criterion': ['gini', 'entropy'],\n",
    "# #     'splitter': ['best', 'random'],\n",
    "# #     'max_depth': [None,1,2,3,4,5, 10],\n",
    "# #     'min_samples_split': [2, 3, 4, 5, 10],\n",
    "# #     'min_samples_leaf': [1, 2, 4,5],\n",
    "# #     'max_features': [None, 'sqrt', 'log2']\n",
    "# # }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
