{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('Training LUNG Cancer Dataset.csv', low_memory=True)\n",
    "# melanoma = pd.read_csv('Melanoma Validation Dataset.csv', low_memory=True)\n",
    "# lancent = pd.read_csv('Lancent_dataset_complete.txt', low_memory=True)\n",
    "extra_lung = pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2099,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpg_sites = train.drop(['geo_accession', 'Labels'], axis=1)\n",
    "labels = train['Labels']\n",
    "\n",
    "# Functions for filling empty values \n",
    "def fill_missing_with_mean(row):\n",
    "    return row.fillna(row.mean())\n",
    "\n",
    "# Dataset conversion for validation\n",
    "lancent_test_standarized = scaler.fit_transform(lancent[rf_cpgs].fillna(0))\n",
    "lancent_y = lancent['Labels']\n",
    "melanoma_test_standarized = scaler.fit_transform(melanoma[rf_cpgs].fillna(0))\n",
    "melanoma_y = melanoma['Labels']\n",
    "\n",
    "# Functions for calling matrices \n",
    "def print_metrics(x, y):\n",
    "    print(\"Accuracy:\", accuracy_score(x, y))\n",
    "    print(\"\\nConfusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier , HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV and ENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Training Dataset for analysis \n",
    "cpg_sites = train.drop(['geo_accession', 'Labels'], axis=1)\n",
    "labels = train['Labels']\n",
    "# Co Effeceint of Variation\n",
    "var = stats.variation(cpg_sites,axis=0)\n",
    "index1 = np.where(var > 1)\n",
    "np.save(\"CV\" + \".npy\", index1[0])\n",
    "res_list = [cpg_sites.columns[i] for i in index1[0].tolist()]\n",
    "# Elastic Net\n",
    "elanet_train = cpg_sites[res_list]\n",
    "enetCV = ElasticNetCV(alphas=[0.0001], l1_ratio=[.1], max_iter=10000).fit(elanet_train, labels)\n",
    "mask = enetCV.coef_ != 0\n",
    "index2 = np.where(mask == True)\n",
    "Enet_CV_CPGs = [cpg_sites.columns[i] for i in index2[0].tolist()]\n",
    "# Splitting based on CPG sites\n",
    "X = cpg_sites[Enet_CV_CPGs]\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"SGD Classifier\"\n",
    "Feature = \"CV + ENET\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGC Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_89 (Dense)            (None, 64)                21952     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30401 (118.75 KB)\n",
      "Trainable params: 30401 (118.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "# from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(64, kernel_initializer='normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.7))\n",
    "# Hidden layers\n",
    "# NN_model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# NN_model.add(Dropout(0.4))\n",
    "NN_model.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['mean_absolute_error', 'accuracy'])\n",
    "NN_model.summary()\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=128, epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "model = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "model.fit(X_train_standardized, y_train)\n",
    "model.score(X_test_standardized,y_test)\n",
    "feature_importances = pd.Series(model.feature_importances_, index=cpg_sites.columns)\n",
    "rf_cpgs = feature_importances[feature_importances > 0].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites[rf_cpgs]\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = type(model).__name__\n",
    "Feature = \"Random Forest\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.5)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGC Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(32, kernel_initializer='normal', input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "NN_model.add(Dropout(0.8))\n",
    "# Hidden layers\n",
    "NN_model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# NN_model.add(Dropout(0.8))\n",
    "# NN_model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer='ADAM', metrics=['mean_absolute_error', 'accuracy'])\n",
    "# NN_model.summary()\n",
    "\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=128, epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SelectFromModel(estimator=RandomForestClassifier())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectFromModel</label><div class=\"sk-toggleable__content\"><pre>SelectFromModel(estimator=RandomForestClassifier())</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "SelectFromModel(estimator=RandomForestClassifier())"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "sel.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_sel_features=sel.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.468345</td>\n",
       "      <td>0.961877</td>\n",
       "      <td>0.873144</td>\n",
       "      <td>-0.514321</td>\n",
       "      <td>2.064817</td>\n",
       "      <td>-0.145675</td>\n",
       "      <td>0.217696</td>\n",
       "      <td>0.263477</td>\n",
       "      <td>0.331939</td>\n",
       "      <td>-1.403186</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044953</td>\n",
       "      <td>0.388860</td>\n",
       "      <td>-2.154744</td>\n",
       "      <td>1.166922</td>\n",
       "      <td>1.131037</td>\n",
       "      <td>-0.890519</td>\n",
       "      <td>1.529835</td>\n",
       "      <td>-1.232949</td>\n",
       "      <td>0.090685</td>\n",
       "      <td>0.055691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.066252</td>\n",
       "      <td>-0.186795</td>\n",
       "      <td>0.523093</td>\n",
       "      <td>-0.790456</td>\n",
       "      <td>1.589893</td>\n",
       "      <td>1.291559</td>\n",
       "      <td>0.269308</td>\n",
       "      <td>-0.500778</td>\n",
       "      <td>1.338485</td>\n",
       "      <td>-1.085779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736773</td>\n",
       "      <td>-0.319725</td>\n",
       "      <td>-1.236041</td>\n",
       "      <td>-1.023107</td>\n",
       "      <td>1.600488</td>\n",
       "      <td>0.044756</td>\n",
       "      <td>-0.760092</td>\n",
       "      <td>-0.238756</td>\n",
       "      <td>-0.299987</td>\n",
       "      <td>0.111686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.046127</td>\n",
       "      <td>-0.320153</td>\n",
       "      <td>0.620740</td>\n",
       "      <td>0.420340</td>\n",
       "      <td>1.542819</td>\n",
       "      <td>0.682589</td>\n",
       "      <td>-0.731935</td>\n",
       "      <td>2.107823</td>\n",
       "      <td>0.191274</td>\n",
       "      <td>0.902899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536238</td>\n",
       "      <td>-0.097872</td>\n",
       "      <td>0.645706</td>\n",
       "      <td>-0.497251</td>\n",
       "      <td>-0.300688</td>\n",
       "      <td>-2.007196</td>\n",
       "      <td>0.283148</td>\n",
       "      <td>-0.248308</td>\n",
       "      <td>-0.229540</td>\n",
       "      <td>-0.453543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.438166</td>\n",
       "      <td>-0.187506</td>\n",
       "      <td>0.272884</td>\n",
       "      <td>3.202991</td>\n",
       "      <td>-0.340843</td>\n",
       "      <td>-0.824004</td>\n",
       "      <td>0.361599</td>\n",
       "      <td>0.946656</td>\n",
       "      <td>0.994114</td>\n",
       "      <td>-1.435395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994723</td>\n",
       "      <td>-0.793743</td>\n",
       "      <td>0.624307</td>\n",
       "      <td>2.695938</td>\n",
       "      <td>0.683187</td>\n",
       "      <td>-0.927865</td>\n",
       "      <td>0.805871</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.953638</td>\n",
       "      <td>0.863869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.309239</td>\n",
       "      <td>-1.041539</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>-0.633602</td>\n",
       "      <td>-0.009333</td>\n",
       "      <td>0.702489</td>\n",
       "      <td>-0.993393</td>\n",
       "      <td>-0.349358</td>\n",
       "      <td>0.498109</td>\n",
       "      <td>-0.313008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480801</td>\n",
       "      <td>-0.317197</td>\n",
       "      <td>-0.651273</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.189521</td>\n",
       "      <td>-0.826415</td>\n",
       "      <td>0.103036</td>\n",
       "      <td>0.392769</td>\n",
       "      <td>-0.268875</td>\n",
       "      <td>0.310314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.203557</td>\n",
       "      <td>-0.107976</td>\n",
       "      <td>-1.225613</td>\n",
       "      <td>-0.282706</td>\n",
       "      <td>-0.369113</td>\n",
       "      <td>0.371529</td>\n",
       "      <td>0.770072</td>\n",
       "      <td>-0.780172</td>\n",
       "      <td>0.087907</td>\n",
       "      <td>0.303605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127647</td>\n",
       "      <td>0.129925</td>\n",
       "      <td>0.684123</td>\n",
       "      <td>-1.003896</td>\n",
       "      <td>0.583457</td>\n",
       "      <td>0.935904</td>\n",
       "      <td>0.564009</td>\n",
       "      <td>0.285819</td>\n",
       "      <td>-1.098610</td>\n",
       "      <td>0.814702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.104622</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>-0.703987</td>\n",
       "      <td>0.374756</td>\n",
       "      <td>1.335409</td>\n",
       "      <td>-0.334483</td>\n",
       "      <td>-0.483122</td>\n",
       "      <td>-1.903233</td>\n",
       "      <td>0.794809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267187</td>\n",
       "      <td>-0.414590</td>\n",
       "      <td>0.771935</td>\n",
       "      <td>0.012635</td>\n",
       "      <td>1.095927</td>\n",
       "      <td>0.113217</td>\n",
       "      <td>0.810632</td>\n",
       "      <td>0.651877</td>\n",
       "      <td>-0.531707</td>\n",
       "      <td>0.015332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.303608</td>\n",
       "      <td>-0.557164</td>\n",
       "      <td>0.600624</td>\n",
       "      <td>0.211757</td>\n",
       "      <td>0.546614</td>\n",
       "      <td>0.981578</td>\n",
       "      <td>-0.134954</td>\n",
       "      <td>-0.123749</td>\n",
       "      <td>0.440579</td>\n",
       "      <td>0.094138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.762387</td>\n",
       "      <td>-0.432199</td>\n",
       "      <td>0.987322</td>\n",
       "      <td>-0.150390</td>\n",
       "      <td>0.036079</td>\n",
       "      <td>0.184241</td>\n",
       "      <td>0.705365</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.377966</td>\n",
       "      <td>0.792129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.087017</td>\n",
       "      <td>0.234965</td>\n",
       "      <td>0.306975</td>\n",
       "      <td>-0.625321</td>\n",
       "      <td>-0.538577</td>\n",
       "      <td>0.779524</td>\n",
       "      <td>0.394970</td>\n",
       "      <td>-0.086393</td>\n",
       "      <td>-0.689710</td>\n",
       "      <td>-0.731322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.432347</td>\n",
       "      <td>-0.443068</td>\n",
       "      <td>1.021329</td>\n",
       "      <td>-0.537280</td>\n",
       "      <td>-0.354667</td>\n",
       "      <td>-1.177600</td>\n",
       "      <td>0.659029</td>\n",
       "      <td>1.004194</td>\n",
       "      <td>0.403447</td>\n",
       "      <td>1.367558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145521</td>\n",
       "      <td>0.152058</td>\n",
       "      <td>-0.459333</td>\n",
       "      <td>0.052201</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.556606</td>\n",
       "      <td>0.337298</td>\n",
       "      <td>-0.316794</td>\n",
       "      <td>-1.061417</td>\n",
       "      <td>0.544924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553787</td>\n",
       "      <td>-0.142016</td>\n",
       "      <td>0.914415</td>\n",
       "      <td>-1.252243</td>\n",
       "      <td>-0.082349</td>\n",
       "      <td>0.292043</td>\n",
       "      <td>0.190434</td>\n",
       "      <td>2.515544</td>\n",
       "      <td>0.575613</td>\n",
       "      <td>0.644237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.139193</td>\n",
       "      <td>-0.831213</td>\n",
       "      <td>0.529474</td>\n",
       "      <td>-0.023723</td>\n",
       "      <td>0.487396</td>\n",
       "      <td>0.585099</td>\n",
       "      <td>-0.180642</td>\n",
       "      <td>-0.138206</td>\n",
       "      <td>-0.286442</td>\n",
       "      <td>-0.924335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.043515</td>\n",
       "      <td>-0.312510</td>\n",
       "      <td>0.010342</td>\n",
       "      <td>1.940610</td>\n",
       "      <td>-0.006238</td>\n",
       "      <td>1.063312</td>\n",
       "      <td>0.099768</td>\n",
       "      <td>-1.214778</td>\n",
       "      <td>0.552570</td>\n",
       "      <td>-0.941951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.517745</td>\n",
       "      <td>0.940279</td>\n",
       "      <td>-0.481184</td>\n",
       "      <td>-0.352718</td>\n",
       "      <td>1.078213</td>\n",
       "      <td>-0.544736</td>\n",
       "      <td>-0.109968</td>\n",
       "      <td>-0.026651</td>\n",
       "      <td>0.319789</td>\n",
       "      <td>0.400743</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.488562</td>\n",
       "      <td>0.325517</td>\n",
       "      <td>-2.137593</td>\n",
       "      <td>-0.104369</td>\n",
       "      <td>0.690180</td>\n",
       "      <td>-0.248975</td>\n",
       "      <td>0.475366</td>\n",
       "      <td>-0.946281</td>\n",
       "      <td>0.196263</td>\n",
       "      <td>-0.082787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.119022</td>\n",
       "      <td>-1.461090</td>\n",
       "      <td>1.100497</td>\n",
       "      <td>-0.519058</td>\n",
       "      <td>0.465462</td>\n",
       "      <td>0.878980</td>\n",
       "      <td>0.082463</td>\n",
       "      <td>-0.358643</td>\n",
       "      <td>0.814442</td>\n",
       "      <td>-0.297973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455880</td>\n",
       "      <td>1.814259</td>\n",
       "      <td>0.184422</td>\n",
       "      <td>1.398047</td>\n",
       "      <td>-0.738260</td>\n",
       "      <td>1.102037</td>\n",
       "      <td>0.677370</td>\n",
       "      <td>0.585452</td>\n",
       "      <td>0.117570</td>\n",
       "      <td>1.788051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.969224</td>\n",
       "      <td>-0.554200</td>\n",
       "      <td>-6.327866</td>\n",
       "      <td>-0.767350</td>\n",
       "      <td>-1.297014</td>\n",
       "      <td>-0.791628</td>\n",
       "      <td>-0.371570</td>\n",
       "      <td>-0.727755</td>\n",
       "      <td>1.080072</td>\n",
       "      <td>0.350536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719149</td>\n",
       "      <td>-0.580009</td>\n",
       "      <td>0.181391</td>\n",
       "      <td>-0.628812</td>\n",
       "      <td>-0.555926</td>\n",
       "      <td>-1.163196</td>\n",
       "      <td>-2.158674</td>\n",
       "      <td>1.046067</td>\n",
       "      <td>-0.340963</td>\n",
       "      <td>-2.843124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.608437</td>\n",
       "      <td>-0.209026</td>\n",
       "      <td>0.194252</td>\n",
       "      <td>-0.814380</td>\n",
       "      <td>-0.332841</td>\n",
       "      <td>-0.357825</td>\n",
       "      <td>-6.165703</td>\n",
       "      <td>-0.406818</td>\n",
       "      <td>-0.784704</td>\n",
       "      <td>-0.759348</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850839</td>\n",
       "      <td>-0.996345</td>\n",
       "      <td>0.585316</td>\n",
       "      <td>-0.246422</td>\n",
       "      <td>-2.635975</td>\n",
       "      <td>1.109899</td>\n",
       "      <td>0.889646</td>\n",
       "      <td>2.381625</td>\n",
       "      <td>0.573255</td>\n",
       "      <td>1.313238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.326546</td>\n",
       "      <td>0.730518</td>\n",
       "      <td>-1.126157</td>\n",
       "      <td>-0.754688</td>\n",
       "      <td>0.013822</td>\n",
       "      <td>0.562408</td>\n",
       "      <td>-0.041053</td>\n",
       "      <td>-0.650994</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>-0.104322</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.064874</td>\n",
       "      <td>-0.466483</td>\n",
       "      <td>1.393496</td>\n",
       "      <td>-1.403715</td>\n",
       "      <td>1.379386</td>\n",
       "      <td>-0.109616</td>\n",
       "      <td>0.546066</td>\n",
       "      <td>-0.924909</td>\n",
       "      <td>-1.224564</td>\n",
       "      <td>0.073066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.359298</td>\n",
       "      <td>0.551866</td>\n",
       "      <td>0.666858</td>\n",
       "      <td>-0.544866</td>\n",
       "      <td>1.677323</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.063398</td>\n",
       "      <td>0.797150</td>\n",
       "      <td>0.317572</td>\n",
       "      <td>-0.424068</td>\n",
       "      <td>...</td>\n",
       "      <td>1.109273</td>\n",
       "      <td>1.453491</td>\n",
       "      <td>-0.139533</td>\n",
       "      <td>0.481557</td>\n",
       "      <td>-0.421575</td>\n",
       "      <td>-1.746287</td>\n",
       "      <td>1.480470</td>\n",
       "      <td>-0.866919</td>\n",
       "      <td>0.979884</td>\n",
       "      <td>0.559317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.884802</td>\n",
       "      <td>0.410563</td>\n",
       "      <td>0.512289</td>\n",
       "      <td>-0.017867</td>\n",
       "      <td>-0.284456</td>\n",
       "      <td>-3.030758</td>\n",
       "      <td>0.239595</td>\n",
       "      <td>-0.061990</td>\n",
       "      <td>1.114974</td>\n",
       "      <td>-0.047296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153761</td>\n",
       "      <td>-0.536793</td>\n",
       "      <td>0.950827</td>\n",
       "      <td>0.143435</td>\n",
       "      <td>-2.294624</td>\n",
       "      <td>1.196131</td>\n",
       "      <td>-2.372160</td>\n",
       "      <td>-0.999612</td>\n",
       "      <td>0.212737</td>\n",
       "      <td>-0.012930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.610742</td>\n",
       "      <td>0.279542</td>\n",
       "      <td>-0.545513</td>\n",
       "      <td>-0.275477</td>\n",
       "      <td>-0.337195</td>\n",
       "      <td>0.488568</td>\n",
       "      <td>0.192033</td>\n",
       "      <td>-0.088704</td>\n",
       "      <td>0.240939</td>\n",
       "      <td>0.792507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722116</td>\n",
       "      <td>-0.289157</td>\n",
       "      <td>-1.022167</td>\n",
       "      <td>-0.144461</td>\n",
       "      <td>0.652486</td>\n",
       "      <td>0.031301</td>\n",
       "      <td>-0.463545</td>\n",
       "      <td>-1.058741</td>\n",
       "      <td>-0.341246</td>\n",
       "      <td>0.644786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.244206</td>\n",
       "      <td>0.965597</td>\n",
       "      <td>-0.806249</td>\n",
       "      <td>-0.467392</td>\n",
       "      <td>0.410591</td>\n",
       "      <td>-0.438830</td>\n",
       "      <td>-0.113727</td>\n",
       "      <td>0.058733</td>\n",
       "      <td>-0.342394</td>\n",
       "      <td>1.248962</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.246516</td>\n",
       "      <td>-0.085461</td>\n",
       "      <td>-0.654631</td>\n",
       "      <td>-0.254895</td>\n",
       "      <td>0.144553</td>\n",
       "      <td>-0.068699</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>1.626570</td>\n",
       "      <td>2.237907</td>\n",
       "      <td>0.210993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.268000</td>\n",
       "      <td>-0.139907</td>\n",
       "      <td>-0.449395</td>\n",
       "      <td>-0.585887</td>\n",
       "      <td>0.595561</td>\n",
       "      <td>-0.876240</td>\n",
       "      <td>-0.543497</td>\n",
       "      <td>0.334983</td>\n",
       "      <td>0.120593</td>\n",
       "      <td>0.211862</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123005</td>\n",
       "      <td>-0.111716</td>\n",
       "      <td>0.955284</td>\n",
       "      <td>0.541505</td>\n",
       "      <td>-0.253161</td>\n",
       "      <td>0.199152</td>\n",
       "      <td>0.026295</td>\n",
       "      <td>-0.822217</td>\n",
       "      <td>1.425170</td>\n",
       "      <td>-0.012027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.824410</td>\n",
       "      <td>-0.397630</td>\n",
       "      <td>0.651541</td>\n",
       "      <td>-0.226962</td>\n",
       "      <td>-0.341528</td>\n",
       "      <td>0.312888</td>\n",
       "      <td>-0.158655</td>\n",
       "      <td>-0.576299</td>\n",
       "      <td>0.960908</td>\n",
       "      <td>-0.175099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195987</td>\n",
       "      <td>0.057815</td>\n",
       "      <td>0.558501</td>\n",
       "      <td>0.047898</td>\n",
       "      <td>0.520552</td>\n",
       "      <td>1.142800</td>\n",
       "      <td>-1.983380</td>\n",
       "      <td>-0.450623</td>\n",
       "      <td>0.759296</td>\n",
       "      <td>0.839734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.763963</td>\n",
       "      <td>0.192401</td>\n",
       "      <td>0.448084</td>\n",
       "      <td>-0.542762</td>\n",
       "      <td>0.153929</td>\n",
       "      <td>-0.781034</td>\n",
       "      <td>-0.493931</td>\n",
       "      <td>-0.268479</td>\n",
       "      <td>0.032037</td>\n",
       "      <td>1.328048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.148145</td>\n",
       "      <td>-0.103433</td>\n",
       "      <td>-2.360376</td>\n",
       "      <td>0.961821</td>\n",
       "      <td>-0.142327</td>\n",
       "      <td>-1.908463</td>\n",
       "      <td>0.693201</td>\n",
       "      <td>-0.325211</td>\n",
       "      <td>-0.887135</td>\n",
       "      <td>-1.654832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.284656</td>\n",
       "      <td>-0.231229</td>\n",
       "      <td>0.333175</td>\n",
       "      <td>-0.343496</td>\n",
       "      <td>0.220950</td>\n",
       "      <td>0.641742</td>\n",
       "      <td>1.710320</td>\n",
       "      <td>-0.159071</td>\n",
       "      <td>0.960967</td>\n",
       "      <td>-0.692926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.685859</td>\n",
       "      <td>-0.508106</td>\n",
       "      <td>0.446362</td>\n",
       "      <td>-0.216957</td>\n",
       "      <td>-1.437294</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>-1.153901</td>\n",
       "      <td>1.079469</td>\n",
       "      <td>-0.241707</td>\n",
       "      <td>0.385623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.945257</td>\n",
       "      <td>-0.021135</td>\n",
       "      <td>-0.076473</td>\n",
       "      <td>-0.617481</td>\n",
       "      <td>0.076306</td>\n",
       "      <td>-0.931108</td>\n",
       "      <td>-0.382048</td>\n",
       "      <td>-0.408251</td>\n",
       "      <td>-2.565938</td>\n",
       "      <td>2.648592</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.212009</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>0.852150</td>\n",
       "      <td>2.523780</td>\n",
       "      <td>-1.285238</td>\n",
       "      <td>-0.067270</td>\n",
       "      <td>-0.897504</td>\n",
       "      <td>2.443284</td>\n",
       "      <td>-0.171860</td>\n",
       "      <td>-2.278283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.854160</td>\n",
       "      <td>-0.298936</td>\n",
       "      <td>0.527060</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>-1.128391</td>\n",
       "      <td>0.218563</td>\n",
       "      <td>0.942686</td>\n",
       "      <td>-0.217358</td>\n",
       "      <td>-0.365779</td>\n",
       "      <td>-0.699775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.527471</td>\n",
       "      <td>-0.502626</td>\n",
       "      <td>0.429862</td>\n",
       "      <td>0.832928</td>\n",
       "      <td>-1.205443</td>\n",
       "      <td>0.978404</td>\n",
       "      <td>-0.224551</td>\n",
       "      <td>-0.273562</td>\n",
       "      <td>0.486103</td>\n",
       "      <td>-1.848926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.137639</td>\n",
       "      <td>-1.192114</td>\n",
       "      <td>0.559532</td>\n",
       "      <td>3.000827</td>\n",
       "      <td>-1.874314</td>\n",
       "      <td>2.308138</td>\n",
       "      <td>1.131515</td>\n",
       "      <td>-0.413509</td>\n",
       "      <td>-2.556876</td>\n",
       "      <td>-0.416053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071003</td>\n",
       "      <td>-0.650087</td>\n",
       "      <td>1.258054</td>\n",
       "      <td>-0.364109</td>\n",
       "      <td>-0.726191</td>\n",
       "      <td>1.107486</td>\n",
       "      <td>-0.315785</td>\n",
       "      <td>-1.029691</td>\n",
       "      <td>-0.137388</td>\n",
       "      <td>1.385896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.921351</td>\n",
       "      <td>-0.424938</td>\n",
       "      <td>0.450495</td>\n",
       "      <td>1.060758</td>\n",
       "      <td>-1.426968</td>\n",
       "      <td>0.369360</td>\n",
       "      <td>0.562767</td>\n",
       "      <td>-0.613913</td>\n",
       "      <td>0.993223</td>\n",
       "      <td>-0.777238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087846</td>\n",
       "      <td>3.042114</td>\n",
       "      <td>0.095016</td>\n",
       "      <td>0.499834</td>\n",
       "      <td>1.303926</td>\n",
       "      <td>0.995660</td>\n",
       "      <td>-2.144002</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.172931</td>\n",
       "      <td>-0.697276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.330254</td>\n",
       "      <td>-0.181926</td>\n",
       "      <td>-0.399608</td>\n",
       "      <td>0.133312</td>\n",
       "      <td>0.435666</td>\n",
       "      <td>0.703532</td>\n",
       "      <td>-1.235438</td>\n",
       "      <td>-0.307879</td>\n",
       "      <td>1.062676</td>\n",
       "      <td>-0.328550</td>\n",
       "      <td>...</td>\n",
       "      <td>1.110122</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>-0.172285</td>\n",
       "      <td>0.188996</td>\n",
       "      <td>0.198070</td>\n",
       "      <td>-0.750083</td>\n",
       "      <td>-0.084595</td>\n",
       "      <td>0.410953</td>\n",
       "      <td>1.618637</td>\n",
       "      <td>-0.110634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.725665</td>\n",
       "      <td>-0.082335</td>\n",
       "      <td>-0.360019</td>\n",
       "      <td>-0.822667</td>\n",
       "      <td>0.101816</td>\n",
       "      <td>1.204092</td>\n",
       "      <td>-0.337925</td>\n",
       "      <td>-0.488256</td>\n",
       "      <td>1.164436</td>\n",
       "      <td>-0.974580</td>\n",
       "      <td>...</td>\n",
       "      <td>1.104412</td>\n",
       "      <td>-0.619594</td>\n",
       "      <td>0.543584</td>\n",
       "      <td>0.407094</td>\n",
       "      <td>0.611556</td>\n",
       "      <td>0.736876</td>\n",
       "      <td>1.531226</td>\n",
       "      <td>-1.548329</td>\n",
       "      <td>-0.239290</td>\n",
       "      <td>0.768947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.200818</td>\n",
       "      <td>-0.542519</td>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.771435</td>\n",
       "      <td>-0.229037</td>\n",
       "      <td>1.543955</td>\n",
       "      <td>-0.210936</td>\n",
       "      <td>0.073468</td>\n",
       "      <td>0.899888</td>\n",
       "      <td>-0.710446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.607129</td>\n",
       "      <td>1.318419</td>\n",
       "      <td>0.623185</td>\n",
       "      <td>-2.485316</td>\n",
       "      <td>1.620704</td>\n",
       "      <td>-0.283523</td>\n",
       "      <td>0.837234</td>\n",
       "      <td>-0.450286</td>\n",
       "      <td>-1.793893</td>\n",
       "      <td>0.026374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.045478</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.292694</td>\n",
       "      <td>-0.668272</td>\n",
       "      <td>-1.423425</td>\n",
       "      <td>-2.279506</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>-0.303814</td>\n",
       "      <td>-0.337137</td>\n",
       "      <td>0.862091</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387979</td>\n",
       "      <td>-0.428401</td>\n",
       "      <td>-0.390115</td>\n",
       "      <td>0.074712</td>\n",
       "      <td>-0.185562</td>\n",
       "      <td>0.930015</td>\n",
       "      <td>0.012071</td>\n",
       "      <td>-1.076205</td>\n",
       "      <td>-0.673049</td>\n",
       "      <td>-0.319628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-1.381248</td>\n",
       "      <td>0.626007</td>\n",
       "      <td>1.037963</td>\n",
       "      <td>-0.523144</td>\n",
       "      <td>-0.277041</td>\n",
       "      <td>-1.544855</td>\n",
       "      <td>-0.492396</td>\n",
       "      <td>0.397311</td>\n",
       "      <td>-2.452593</td>\n",
       "      <td>-1.026513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202611</td>\n",
       "      <td>4.654264</td>\n",
       "      <td>1.545132</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>-2.109601</td>\n",
       "      <td>1.261469</td>\n",
       "      <td>2.586153</td>\n",
       "      <td>-0.382131</td>\n",
       "      <td>1.362311</td>\n",
       "      <td>0.768245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.016018</td>\n",
       "      <td>-0.992527</td>\n",
       "      <td>0.529714</td>\n",
       "      <td>2.325461</td>\n",
       "      <td>1.949224</td>\n",
       "      <td>-0.671705</td>\n",
       "      <td>-0.931739</td>\n",
       "      <td>-0.322759</td>\n",
       "      <td>0.145803</td>\n",
       "      <td>2.465084</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.166989</td>\n",
       "      <td>2.676605</td>\n",
       "      <td>0.767184</td>\n",
       "      <td>1.047360</td>\n",
       "      <td>-0.327523</td>\n",
       "      <td>0.866535</td>\n",
       "      <td>-0.611067</td>\n",
       "      <td>0.709982</td>\n",
       "      <td>1.282063</td>\n",
       "      <td>-3.303747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.312821</td>\n",
       "      <td>1.317164</td>\n",
       "      <td>-0.830845</td>\n",
       "      <td>-0.685991</td>\n",
       "      <td>0.332855</td>\n",
       "      <td>0.380420</td>\n",
       "      <td>-0.237760</td>\n",
       "      <td>-0.285371</td>\n",
       "      <td>0.568799</td>\n",
       "      <td>-0.281416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033636</td>\n",
       "      <td>-0.114102</td>\n",
       "      <td>0.559787</td>\n",
       "      <td>-1.861512</td>\n",
       "      <td>0.844525</td>\n",
       "      <td>0.036370</td>\n",
       "      <td>0.446342</td>\n",
       "      <td>0.314849</td>\n",
       "      <td>0.271055</td>\n",
       "      <td>-0.532813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-1.238341</td>\n",
       "      <td>-0.634568</td>\n",
       "      <td>0.548110</td>\n",
       "      <td>-0.186700</td>\n",
       "      <td>-0.711015</td>\n",
       "      <td>-0.760183</td>\n",
       "      <td>0.345533</td>\n",
       "      <td>-0.147953</td>\n",
       "      <td>-0.321544</td>\n",
       "      <td>2.275867</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.330954</td>\n",
       "      <td>-0.732387</td>\n",
       "      <td>0.416045</td>\n",
       "      <td>-0.225219</td>\n",
       "      <td>-1.768187</td>\n",
       "      <td>1.245896</td>\n",
       "      <td>-0.927493</td>\n",
       "      <td>-0.640630</td>\n",
       "      <td>1.812766</td>\n",
       "      <td>0.718375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.011058</td>\n",
       "      <td>-0.134012</td>\n",
       "      <td>-0.486240</td>\n",
       "      <td>-0.688488</td>\n",
       "      <td>-0.494558</td>\n",
       "      <td>-0.321116</td>\n",
       "      <td>0.276341</td>\n",
       "      <td>-0.404050</td>\n",
       "      <td>-0.559562</td>\n",
       "      <td>0.364398</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.044725</td>\n",
       "      <td>-0.471320</td>\n",
       "      <td>0.262435</td>\n",
       "      <td>0.359721</td>\n",
       "      <td>-1.396645</td>\n",
       "      <td>0.155956</td>\n",
       "      <td>-0.703042</td>\n",
       "      <td>0.760250</td>\n",
       "      <td>1.297756</td>\n",
       "      <td>-1.114401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.918739</td>\n",
       "      <td>5.967793</td>\n",
       "      <td>0.051552</td>\n",
       "      <td>3.417922</td>\n",
       "      <td>-0.585390</td>\n",
       "      <td>-1.020096</td>\n",
       "      <td>-0.008235</td>\n",
       "      <td>-0.972401</td>\n",
       "      <td>1.085281</td>\n",
       "      <td>0.069042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.484922</td>\n",
       "      <td>-0.138037</td>\n",
       "      <td>1.094639</td>\n",
       "      <td>-1.313783</td>\n",
       "      <td>-1.409975</td>\n",
       "      <td>0.958414</td>\n",
       "      <td>-1.225443</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>-0.759197</td>\n",
       "      <td>0.504428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.094814</td>\n",
       "      <td>-0.485621</td>\n",
       "      <td>-0.116393</td>\n",
       "      <td>-0.608102</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>0.544140</td>\n",
       "      <td>-0.043786</td>\n",
       "      <td>-0.407050</td>\n",
       "      <td>0.115272</td>\n",
       "      <td>-1.315706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535295</td>\n",
       "      <td>-0.116630</td>\n",
       "      <td>-1.491523</td>\n",
       "      <td>-0.912289</td>\n",
       "      <td>0.303727</td>\n",
       "      <td>-0.534743</td>\n",
       "      <td>-0.060109</td>\n",
       "      <td>-0.170359</td>\n",
       "      <td>-2.022562</td>\n",
       "      <td>-0.301322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-1.016467</td>\n",
       "      <td>-0.830169</td>\n",
       "      <td>1.002107</td>\n",
       "      <td>1.671710</td>\n",
       "      <td>0.027076</td>\n",
       "      <td>0.675440</td>\n",
       "      <td>1.139880</td>\n",
       "      <td>-0.369356</td>\n",
       "      <td>-0.730226</td>\n",
       "      <td>2.178579</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.522683</td>\n",
       "      <td>-0.887820</td>\n",
       "      <td>1.081387</td>\n",
       "      <td>0.079484</td>\n",
       "      <td>0.466996</td>\n",
       "      <td>0.792390</td>\n",
       "      <td>-1.567516</td>\n",
       "      <td>1.905848</td>\n",
       "      <td>0.531075</td>\n",
       "      <td>0.928946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.723143</td>\n",
       "      <td>-0.555227</td>\n",
       "      <td>-0.569322</td>\n",
       "      <td>0.885521</td>\n",
       "      <td>-0.096578</td>\n",
       "      <td>0.525025</td>\n",
       "      <td>-0.245770</td>\n",
       "      <td>1.197542</td>\n",
       "      <td>-0.679343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709944</td>\n",
       "      <td>-0.408392</td>\n",
       "      <td>-0.217589</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.850215</td>\n",
       "      <td>-0.071967</td>\n",
       "      <td>-0.941385</td>\n",
       "      <td>-1.130990</td>\n",
       "      <td>-1.309602</td>\n",
       "      <td>0.059633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.919803</td>\n",
       "      <td>0.691707</td>\n",
       "      <td>0.940083</td>\n",
       "      <td>0.268925</td>\n",
       "      <td>2.335547</td>\n",
       "      <td>1.013785</td>\n",
       "      <td>-0.628996</td>\n",
       "      <td>4.668623</td>\n",
       "      <td>-0.289517</td>\n",
       "      <td>2.178436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.525210</td>\n",
       "      <td>1.831573</td>\n",
       "      <td>-0.251107</td>\n",
       "      <td>-0.749466</td>\n",
       "      <td>-0.044227</td>\n",
       "      <td>-2.218069</td>\n",
       "      <td>0.437456</td>\n",
       "      <td>1.170048</td>\n",
       "      <td>0.450166</td>\n",
       "      <td>0.980280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.383029</td>\n",
       "      <td>-0.749052</td>\n",
       "      <td>1.384740</td>\n",
       "      <td>1.426213</td>\n",
       "      <td>1.206688</td>\n",
       "      <td>1.090210</td>\n",
       "      <td>-1.100946</td>\n",
       "      <td>0.592848</td>\n",
       "      <td>-0.656407</td>\n",
       "      <td>1.266939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.311397</td>\n",
       "      <td>-0.590479</td>\n",
       "      <td>-0.089206</td>\n",
       "      <td>1.032531</td>\n",
       "      <td>0.730341</td>\n",
       "      <td>-1.753375</td>\n",
       "      <td>1.135299</td>\n",
       "      <td>0.798803</td>\n",
       "      <td>1.164539</td>\n",
       "      <td>0.410870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.987682</td>\n",
       "      <td>0.789723</td>\n",
       "      <td>-0.725222</td>\n",
       "      <td>-0.104486</td>\n",
       "      <td>0.318614</td>\n",
       "      <td>-0.366373</td>\n",
       "      <td>-0.379533</td>\n",
       "      <td>-0.168513</td>\n",
       "      <td>0.297904</td>\n",
       "      <td>-0.347739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.223146</td>\n",
       "      <td>0.074250</td>\n",
       "      <td>-1.429544</td>\n",
       "      <td>0.272023</td>\n",
       "      <td>0.711725</td>\n",
       "      <td>-0.071517</td>\n",
       "      <td>0.211145</td>\n",
       "      <td>-0.074202</td>\n",
       "      <td>1.016763</td>\n",
       "      <td>-1.091769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.681699</td>\n",
       "      <td>-0.891222</td>\n",
       "      <td>0.075278</td>\n",
       "      <td>-0.518714</td>\n",
       "      <td>-0.492746</td>\n",
       "      <td>-2.073884</td>\n",
       "      <td>1.034398</td>\n",
       "      <td>-0.658565</td>\n",
       "      <td>-0.649638</td>\n",
       "      <td>-0.186893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676425</td>\n",
       "      <td>-0.530874</td>\n",
       "      <td>-1.273826</td>\n",
       "      <td>1.566829</td>\n",
       "      <td>0.654962</td>\n",
       "      <td>0.768243</td>\n",
       "      <td>0.087504</td>\n",
       "      <td>0.462239</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.574085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.844693</td>\n",
       "      <td>0.533396</td>\n",
       "      <td>-0.746004</td>\n",
       "      <td>-0.290886</td>\n",
       "      <td>1.211992</td>\n",
       "      <td>0.381074</td>\n",
       "      <td>0.312299</td>\n",
       "      <td>-0.537850</td>\n",
       "      <td>0.709408</td>\n",
       "      <td>-0.622222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.330613</td>\n",
       "      <td>-0.318799</td>\n",
       "      <td>0.512901</td>\n",
       "      <td>-0.749641</td>\n",
       "      <td>1.115714</td>\n",
       "      <td>0.521213</td>\n",
       "      <td>-1.115568</td>\n",
       "      <td>0.741069</td>\n",
       "      <td>-0.331798</td>\n",
       "      <td>0.229410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.062850</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>-0.716741</td>\n",
       "      <td>-0.723620</td>\n",
       "      <td>0.284323</td>\n",
       "      <td>0.386449</td>\n",
       "      <td>-0.637119</td>\n",
       "      <td>-0.195678</td>\n",
       "      <td>-0.846178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602512</td>\n",
       "      <td>-0.085883</td>\n",
       "      <td>0.292954</td>\n",
       "      <td>-0.258877</td>\n",
       "      <td>0.464914</td>\n",
       "      <td>0.689402</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>-0.373452</td>\n",
       "      <td>0.230564</td>\n",
       "      <td>1.122755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.834491</td>\n",
       "      <td>-0.699594</td>\n",
       "      <td>0.598131</td>\n",
       "      <td>-0.199376</td>\n",
       "      <td>1.371056</td>\n",
       "      <td>0.723935</td>\n",
       "      <td>0.181465</td>\n",
       "      <td>1.200961</td>\n",
       "      <td>-0.202022</td>\n",
       "      <td>0.990105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057878</td>\n",
       "      <td>-0.264116</td>\n",
       "      <td>-1.168118</td>\n",
       "      <td>1.331192</td>\n",
       "      <td>0.958600</td>\n",
       "      <td>-0.018619</td>\n",
       "      <td>-0.282975</td>\n",
       "      <td>-0.278644</td>\n",
       "      <td>-0.356848</td>\n",
       "      <td>-0.154458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-1.101958</td>\n",
       "      <td>-0.183704</td>\n",
       "      <td>0.303296</td>\n",
       "      <td>-0.318342</td>\n",
       "      <td>-0.673378</td>\n",
       "      <td>0.550684</td>\n",
       "      <td>0.358195</td>\n",
       "      <td>-0.552491</td>\n",
       "      <td>0.550992</td>\n",
       "      <td>-0.018437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489434</td>\n",
       "      <td>-1.127910</td>\n",
       "      <td>0.984730</td>\n",
       "      <td>-0.433391</td>\n",
       "      <td>1.552780</td>\n",
       "      <td>0.746527</td>\n",
       "      <td>-0.798385</td>\n",
       "      <td>0.394120</td>\n",
       "      <td>-0.326934</td>\n",
       "      <td>0.924987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.430059</td>\n",
       "      <td>-0.175630</td>\n",
       "      <td>-0.401965</td>\n",
       "      <td>-0.753325</td>\n",
       "      <td>-0.372894</td>\n",
       "      <td>0.589955</td>\n",
       "      <td>0.640618</td>\n",
       "      <td>-0.661840</td>\n",
       "      <td>1.556555</td>\n",
       "      <td>-2.132869</td>\n",
       "      <td>...</td>\n",
       "      <td>0.626780</td>\n",
       "      <td>0.352793</td>\n",
       "      <td>0.846648</td>\n",
       "      <td>-0.365493</td>\n",
       "      <td>1.357958</td>\n",
       "      <td>0.098578</td>\n",
       "      <td>-0.070107</td>\n",
       "      <td>-0.340895</td>\n",
       "      <td>-2.261890</td>\n",
       "      <td>0.261115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-0.490711</td>\n",
       "      <td>-0.019985</td>\n",
       "      <td>0.245387</td>\n",
       "      <td>-0.779812</td>\n",
       "      <td>-1.375247</td>\n",
       "      <td>-2.209027</td>\n",
       "      <td>0.074822</td>\n",
       "      <td>-0.379931</td>\n",
       "      <td>-1.288042</td>\n",
       "      <td>-0.035717</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.788851</td>\n",
       "      <td>-0.542848</td>\n",
       "      <td>-0.934400</td>\n",
       "      <td>-1.157434</td>\n",
       "      <td>-1.155087</td>\n",
       "      <td>0.386314</td>\n",
       "      <td>-0.286913</td>\n",
       "      <td>-1.110409</td>\n",
       "      <td>0.372993</td>\n",
       "      <td>-0.036534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-0.946898</td>\n",
       "      <td>-1.022953</td>\n",
       "      <td>0.051959</td>\n",
       "      <td>0.429135</td>\n",
       "      <td>-0.026397</td>\n",
       "      <td>-0.917515</td>\n",
       "      <td>0.553416</td>\n",
       "      <td>-0.329108</td>\n",
       "      <td>-0.754083</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128578</td>\n",
       "      <td>-0.225194</td>\n",
       "      <td>-0.513659</td>\n",
       "      <td>0.082654</td>\n",
       "      <td>-0.721036</td>\n",
       "      <td>0.682624</td>\n",
       "      <td>1.126328</td>\n",
       "      <td>0.479468</td>\n",
       "      <td>-1.442462</td>\n",
       "      <td>0.278994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.498364</td>\n",
       "      <td>-0.737417</td>\n",
       "      <td>-0.411646</td>\n",
       "      <td>-0.552739</td>\n",
       "      <td>-0.754072</td>\n",
       "      <td>-1.463506</td>\n",
       "      <td>-0.724692</td>\n",
       "      <td>-0.584496</td>\n",
       "      <td>-2.506218</td>\n",
       "      <td>-0.559483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299637</td>\n",
       "      <td>0.343291</td>\n",
       "      <td>-0.000123</td>\n",
       "      <td>1.235381</td>\n",
       "      <td>-0.605020</td>\n",
       "      <td>-2.766362</td>\n",
       "      <td>-0.710676</td>\n",
       "      <td>0.480753</td>\n",
       "      <td>1.941014</td>\n",
       "      <td>-2.107872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-0.584919</td>\n",
       "      <td>-0.310870</td>\n",
       "      <td>-0.119476</td>\n",
       "      <td>1.322214</td>\n",
       "      <td>0.353631</td>\n",
       "      <td>-0.141365</td>\n",
       "      <td>0.987637</td>\n",
       "      <td>3.199988</td>\n",
       "      <td>0.520822</td>\n",
       "      <td>-1.091373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.916403</td>\n",
       "      <td>-0.193330</td>\n",
       "      <td>-0.026129</td>\n",
       "      <td>-0.581729</td>\n",
       "      <td>0.682308</td>\n",
       "      <td>-1.642583</td>\n",
       "      <td>0.262188</td>\n",
       "      <td>-0.403934</td>\n",
       "      <td>1.294226</td>\n",
       "      <td>0.156651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.162879</td>\n",
       "      <td>0.020175</td>\n",
       "      <td>-0.158050</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>-0.134330</td>\n",
       "      <td>0.011794</td>\n",
       "      <td>0.166630</td>\n",
       "      <td>1.096313</td>\n",
       "      <td>-0.033198</td>\n",
       "      <td>0.053129</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460332</td>\n",
       "      <td>-0.525820</td>\n",
       "      <td>-3.299305</td>\n",
       "      <td>0.736223</td>\n",
       "      <td>0.672375</td>\n",
       "      <td>0.020585</td>\n",
       "      <td>0.538960</td>\n",
       "      <td>0.527446</td>\n",
       "      <td>-1.808974</td>\n",
       "      <td>-0.407521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.322335</td>\n",
       "      <td>1.658520</td>\n",
       "      <td>0.391860</td>\n",
       "      <td>1.381086</td>\n",
       "      <td>-0.744110</td>\n",
       "      <td>0.101312</td>\n",
       "      <td>-0.563813</td>\n",
       "      <td>-0.458070</td>\n",
       "      <td>-0.299595</td>\n",
       "      <td>0.238980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765881</td>\n",
       "      <td>-0.805719</td>\n",
       "      <td>-0.145340</td>\n",
       "      <td>-0.523912</td>\n",
       "      <td>-1.237360</td>\n",
       "      <td>0.153206</td>\n",
       "      <td>0.197136</td>\n",
       "      <td>0.763262</td>\n",
       "      <td>-0.491389</td>\n",
       "      <td>-0.816772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.267940</td>\n",
       "      <td>-0.273065</td>\n",
       "      <td>0.366455</td>\n",
       "      <td>-0.255555</td>\n",
       "      <td>-2.838450</td>\n",
       "      <td>0.160264</td>\n",
       "      <td>0.701836</td>\n",
       "      <td>-0.466672</td>\n",
       "      <td>0.937428</td>\n",
       "      <td>-0.696945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.987230</td>\n",
       "      <td>-0.037183</td>\n",
       "      <td>-0.522682</td>\n",
       "      <td>0.274993</td>\n",
       "      <td>0.548657</td>\n",
       "      <td>0.089122</td>\n",
       "      <td>-0.084421</td>\n",
       "      <td>-0.727874</td>\n",
       "      <td>0.522598</td>\n",
       "      <td>-0.109135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>-0.325401</td>\n",
       "      <td>-0.330154</td>\n",
       "      <td>-0.234444</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>-1.073280</td>\n",
       "      <td>-0.758271</td>\n",
       "      <td>0.795963</td>\n",
       "      <td>2.984245</td>\n",
       "      <td>-1.415340</td>\n",
       "      <td>-0.134489</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.589478</td>\n",
       "      <td>-0.661429</td>\n",
       "      <td>0.301602</td>\n",
       "      <td>-2.048177</td>\n",
       "      <td>0.080675</td>\n",
       "      <td>-1.411915</td>\n",
       "      <td>-0.578436</td>\n",
       "      <td>-0.367693</td>\n",
       "      <td>-1.643935</td>\n",
       "      <td>0.280959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>-0.088134</td>\n",
       "      <td>0.443159</td>\n",
       "      <td>-0.151061</td>\n",
       "      <td>-0.733901</td>\n",
       "      <td>-0.351973</td>\n",
       "      <td>-0.540345</td>\n",
       "      <td>0.457193</td>\n",
       "      <td>-0.425823</td>\n",
       "      <td>-0.221454</td>\n",
       "      <td>0.170288</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.136238</td>\n",
       "      <td>-0.116378</td>\n",
       "      <td>-1.132216</td>\n",
       "      <td>-0.684635</td>\n",
       "      <td>-0.487742</td>\n",
       "      <td>-0.946363</td>\n",
       "      <td>0.246197</td>\n",
       "      <td>0.691111</td>\n",
       "      <td>-0.435208</td>\n",
       "      <td>-0.303709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-1.882262</td>\n",
       "      <td>-0.513184</td>\n",
       "      <td>0.227113</td>\n",
       "      <td>-0.717102</td>\n",
       "      <td>-1.616959</td>\n",
       "      <td>0.820214</td>\n",
       "      <td>0.693190</td>\n",
       "      <td>-0.850433</td>\n",
       "      <td>0.828745</td>\n",
       "      <td>-0.684563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148944</td>\n",
       "      <td>-0.559822</td>\n",
       "      <td>0.031149</td>\n",
       "      <td>-0.434878</td>\n",
       "      <td>-0.549658</td>\n",
       "      <td>1.110178</td>\n",
       "      <td>2.432223</td>\n",
       "      <td>-0.520495</td>\n",
       "      <td>-0.723350</td>\n",
       "      <td>0.364719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows × 369 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0   1.468345  0.961877  0.873144 -0.514321  2.064817 -0.145675  0.217696   \n",
       "1   2.066252 -0.186795  0.523093 -0.790456  1.589893  1.291559  0.269308   \n",
       "2  -0.046127 -0.320153  0.620740  0.420340  1.542819  0.682589 -0.731935   \n",
       "3  -1.438166 -0.187506  0.272884  3.202991 -0.340843 -0.824004  0.361599   \n",
       "4  -0.309239 -1.041539  0.008472 -0.633602 -0.009333  0.702489 -0.993393   \n",
       "5   0.203557 -0.107976 -1.225613 -0.282706 -0.369113  0.371529  0.770072   \n",
       "6  -1.104622  0.026245 -0.080639 -0.703987  0.374756  1.335409 -0.334483   \n",
       "7   0.303608 -0.557164  0.600624  0.211757  0.546614  0.981578 -0.134954   \n",
       "8  -0.087017  0.234965  0.306975 -0.625321 -0.538577  0.779524  0.394970   \n",
       "9   0.145521  0.152058 -0.459333  0.052201  0.738600  0.556606  0.337298   \n",
       "10  0.139193 -0.831213  0.529474 -0.023723  0.487396  0.585099 -0.180642   \n",
       "11  1.517745  0.940279 -0.481184 -0.352718  1.078213 -0.544736 -0.109968   \n",
       "12  2.119022 -1.461090  1.100497 -0.519058  0.465462  0.878980  0.082463   \n",
       "13 -1.969224 -0.554200 -6.327866 -0.767350 -1.297014 -0.791628 -0.371570   \n",
       "14 -1.608437 -0.209026  0.194252 -0.814380 -0.332841 -0.357825 -6.165703   \n",
       "15 -0.326546  0.730518 -1.126157 -0.754688  0.013822  0.562408 -0.041053   \n",
       "16  0.359298  0.551866  0.666858 -0.544866  1.677323  0.040003  0.063398   \n",
       "17  0.884802  0.410563  0.512289 -0.017867 -0.284456 -3.030758  0.239595   \n",
       "18 -0.610742  0.279542 -0.545513 -0.275477 -0.337195  0.488568  0.192033   \n",
       "19  0.244206  0.965597 -0.806249 -0.467392  0.410591 -0.438830 -0.113727   \n",
       "20 -0.268000 -0.139907 -0.449395 -0.585887  0.595561 -0.876240 -0.543497   \n",
       "21  1.824410 -0.397630  0.651541 -0.226962 -0.341528  0.312888 -0.158655   \n",
       "22  0.763963  0.192401  0.448084 -0.542762  0.153929 -0.781034 -0.493931   \n",
       "23  1.284656 -0.231229  0.333175 -0.343496  0.220950  0.641742  1.710320   \n",
       "24 -0.945257 -0.021135 -0.076473 -0.617481  0.076306 -0.931108 -0.382048   \n",
       "25 -0.854160 -0.298936  0.527060  0.007615 -1.128391  0.218563  0.942686   \n",
       "26  1.137639 -1.192114  0.559532  3.000827 -1.874314  2.308138  1.131515   \n",
       "27  1.921351 -0.424938  0.450495  1.060758 -1.426968  0.369360  0.562767   \n",
       "28 -0.330254 -0.181926 -0.399608  0.133312  0.435666  0.703532 -1.235438   \n",
       "29 -0.725665 -0.082335 -0.360019 -0.822667  0.101816  1.204092 -0.337925   \n",
       "30 -0.200818 -0.542519  0.622549  0.771435 -0.229037  1.543955 -0.210936   \n",
       "31 -1.045478 -0.267849 -0.292694 -0.668272 -1.423425 -2.279506  0.745146   \n",
       "32 -1.381248  0.626007  1.037963 -0.523144 -0.277041 -1.544855 -0.492396   \n",
       "33  0.016018 -0.992527  0.529714  2.325461  1.949224 -0.671705 -0.931739   \n",
       "34 -0.312821  1.317164 -0.830845 -0.685991  0.332855  0.380420 -0.237760   \n",
       "35 -1.238341 -0.634568  0.548110 -0.186700 -0.711015 -0.760183  0.345533   \n",
       "36  0.011058 -0.134012 -0.486240 -0.688488 -0.494558 -0.321116  0.276341   \n",
       "37  1.918739  5.967793  0.051552  3.417922 -0.585390 -1.020096 -0.008235   \n",
       "38 -0.094814 -0.485621 -0.116393 -0.608102  0.611756  0.544140 -0.043786   \n",
       "39 -1.016467 -0.830169  1.002107  1.671710  0.027076  0.675440  1.139880   \n",
       "40  0.437700  0.723143 -0.555227 -0.569322  0.885521 -0.096578  0.525025   \n",
       "41 -0.919803  0.691707  0.940083  0.268925  2.335547  1.013785 -0.628996   \n",
       "42 -0.383029 -0.749052  1.384740  1.426213  1.206688  1.090210 -1.100946   \n",
       "43  0.987682  0.789723 -0.725222 -0.104486  0.318614 -0.366373 -0.379533   \n",
       "44 -0.681699 -0.891222  0.075278 -0.518714 -0.492746 -2.073884  1.034398   \n",
       "45  0.844693  0.533396 -0.746004 -0.290886  1.211992  0.381074  0.312299   \n",
       "46  0.062850  0.004211  0.011833 -0.716741 -0.723620  0.284323  0.386449   \n",
       "47  0.834491 -0.699594  0.598131 -0.199376  1.371056  0.723935  0.181465   \n",
       "48 -1.101958 -0.183704  0.303296 -0.318342 -0.673378  0.550684  0.358195   \n",
       "49 -0.430059 -0.175630 -0.401965 -0.753325 -0.372894  0.589955  0.640618   \n",
       "50 -0.490711 -0.019985  0.245387 -0.779812 -1.375247 -2.209027  0.074822   \n",
       "51 -0.946898 -1.022953  0.051959  0.429135 -0.026397 -0.917515  0.553416   \n",
       "52  1.498364 -0.737417 -0.411646 -0.552739 -0.754072 -1.463506 -0.724692   \n",
       "53 -0.584919 -0.310870 -0.119476  1.322214  0.353631 -0.141365  0.987637   \n",
       "54  0.162879  0.020175 -0.158050  0.003217 -0.134330  0.011794  0.166630   \n",
       "55  0.322335  1.658520  0.391860  1.381086 -0.744110  0.101312 -0.563813   \n",
       "56  0.267940 -0.273065  0.366455 -0.255555 -2.838450  0.160264  0.701836   \n",
       "57 -0.325401 -0.330154 -0.234444 -0.008952 -1.073280 -0.758271  0.795963   \n",
       "58 -0.088134  0.443159 -0.151061 -0.733901 -0.351973 -0.540345  0.457193   \n",
       "59 -1.882262 -0.513184  0.227113 -0.717102 -1.616959  0.820214  0.693190   \n",
       "\n",
       "         7         8         9    ...       359       360       361       362  \\\n",
       "0   0.263477  0.331939 -1.403186  ...  1.044953  0.388860 -2.154744  1.166922   \n",
       "1  -0.500778  1.338485 -1.085779  ...  0.736773 -0.319725 -1.236041 -1.023107   \n",
       "2   2.107823  0.191274  0.902899  ...  0.536238 -0.097872  0.645706 -0.497251   \n",
       "3   0.946656  0.994114 -1.435395  ...  0.994723 -0.793743  0.624307  2.695938   \n",
       "4  -0.349358  0.498109 -0.313008  ...  0.480801 -0.317197 -0.651273  0.673200   \n",
       "5  -0.780172  0.087907  0.303605  ... -0.127647  0.129925  0.684123 -1.003896   \n",
       "6  -0.483122 -1.903233  0.794809  ... -0.267187 -0.414590  0.771935  0.012635   \n",
       "7  -0.123749  0.440579  0.094138  ...  0.762387 -0.432199  0.987322 -0.150390   \n",
       "8  -0.086393 -0.689710 -0.731322  ... -0.432347 -0.443068  1.021329 -0.537280   \n",
       "9  -0.316794 -1.061417  0.544924  ...  0.553787 -0.142016  0.914415 -1.252243   \n",
       "10 -0.138206 -0.286442 -0.924335  ...  1.043515 -0.312510  0.010342  1.940610   \n",
       "11 -0.026651  0.319789  0.400743  ... -0.488562  0.325517 -2.137593 -0.104369   \n",
       "12 -0.358643  0.814442 -0.297973  ...  0.455880  1.814259  0.184422  1.398047   \n",
       "13 -0.727755  1.080072  0.350536  ...  0.719149 -0.580009  0.181391 -0.628812   \n",
       "14 -0.406818 -0.784704 -0.759348  ...  0.850839 -0.996345  0.585316 -0.246422   \n",
       "15 -0.650994  0.989272 -0.104322  ... -0.064874 -0.466483  1.393496 -1.403715   \n",
       "16  0.797150  0.317572 -0.424068  ...  1.109273  1.453491 -0.139533  0.481557   \n",
       "17 -0.061990  1.114974 -0.047296  ...  0.153761 -0.536793  0.950827  0.143435   \n",
       "18 -0.088704  0.240939  0.792507  ... -0.722116 -0.289157 -1.022167 -0.144461   \n",
       "19  0.058733 -0.342394  1.248962  ... -1.246516 -0.085461 -0.654631 -0.254895   \n",
       "20  0.334983  0.120593  0.211862  ... -0.123005 -0.111716  0.955284  0.541505   \n",
       "21 -0.576299  0.960908 -0.175099  ...  0.195987  0.057815  0.558501  0.047898   \n",
       "22 -0.268479  0.032037  1.328048  ... -0.148145 -0.103433 -2.360376  0.961821   \n",
       "23 -0.159071  0.960967 -0.692926  ...  0.685859 -0.508106  0.446362 -0.216957   \n",
       "24 -0.408251 -2.565938  2.648592  ... -1.212009  0.013169  0.852150  2.523780   \n",
       "25 -0.217358 -0.365779 -0.699775  ...  0.527471 -0.502626  0.429862  0.832928   \n",
       "26 -0.413509 -2.556876 -0.416053  ...  0.071003 -0.650087  1.258054 -0.364109   \n",
       "27 -0.613913  0.993223 -0.777238  ...  0.087846  3.042114  0.095016  0.499834   \n",
       "28 -0.307879  1.062676 -0.328550  ...  1.110122  0.129252 -0.172285  0.188996   \n",
       "29 -0.488256  1.164436 -0.974580  ...  1.104412 -0.619594  0.543584  0.407094   \n",
       "30  0.073468  0.899888 -0.710446  ...  0.607129  1.318419  0.623185 -2.485316   \n",
       "31 -0.303814 -0.337137  0.862091  ... -1.387979 -0.428401 -0.390115  0.074712   \n",
       "32  0.397311 -2.452593 -1.026513  ...  1.202611  4.654264  1.545132  0.002539   \n",
       "33 -0.322759  0.145803  2.465084  ... -2.166989  2.676605  0.767184  1.047360   \n",
       "34 -0.285371  0.568799 -0.281416  ...  0.033636 -0.114102  0.559787 -1.861512   \n",
       "35 -0.147953 -0.321544  2.275867  ... -3.330954 -0.732387  0.416045 -0.225219   \n",
       "36 -0.404050 -0.559562  0.364398  ... -1.044725 -0.471320  0.262435  0.359721   \n",
       "37 -0.972401  1.085281  0.069042  ...  0.484922 -0.138037  1.094639 -1.313783   \n",
       "38 -0.407050  0.115272 -1.315706  ...  0.535295 -0.116630 -1.491523 -0.912289   \n",
       "39 -0.369356 -0.730226  2.178579  ... -0.522683 -0.887820  1.081387  0.079484   \n",
       "40 -0.245770  1.197542 -0.679343  ...  0.709944 -0.408392 -0.217589  0.001814   \n",
       "41  4.668623 -0.289517  2.178436  ... -0.525210  1.831573 -0.251107 -0.749466   \n",
       "42  0.592848 -0.656407  1.266939  ...  0.311397 -0.590479 -0.089206  1.032531   \n",
       "43 -0.168513  0.297904 -0.347739  ... -0.223146  0.074250 -1.429544  0.272023   \n",
       "44 -0.658565 -0.649638 -0.186893  ...  0.676425 -0.530874 -1.273826  1.566829   \n",
       "45 -0.537850  0.709408 -0.622222  ...  0.330613 -0.318799  0.512901 -0.749641   \n",
       "46 -0.637119 -0.195678 -0.846178  ...  0.602512 -0.085883  0.292954 -0.258877   \n",
       "47  1.200961 -0.202022  0.990105  ...  0.057878 -0.264116 -1.168118  1.331192   \n",
       "48 -0.552491  0.550992 -0.018437  ...  0.489434 -1.127910  0.984730 -0.433391   \n",
       "49 -0.661840  1.556555 -2.132869  ...  0.626780  0.352793  0.846648 -0.365493   \n",
       "50 -0.379931 -1.288042 -0.035717  ... -3.788851 -0.542848 -0.934400 -1.157434   \n",
       "51 -0.329108 -0.754083  0.226020  ...  0.128578 -0.225194 -0.513659  0.082654   \n",
       "52 -0.584496 -2.506218 -0.559483  ... -0.299637  0.343291 -0.000123  1.235381   \n",
       "53  3.199988  0.520822 -1.091373  ...  0.916403 -0.193330 -0.026129 -0.581729   \n",
       "54  1.096313 -0.033198  0.053129  ... -0.460332 -0.525820 -3.299305  0.736223   \n",
       "55 -0.458070 -0.299595  0.238980  ... -0.765881 -0.805719 -0.145340 -0.523912   \n",
       "56 -0.466672  0.937428 -0.696945  ...  0.987230 -0.037183 -0.522682  0.274993   \n",
       "57  2.984245 -1.415340 -0.134489  ... -1.589478 -0.661429  0.301602 -2.048177   \n",
       "58 -0.425823 -0.221454  0.170288  ... -1.136238 -0.116378 -1.132216 -0.684635   \n",
       "59 -0.850433  0.828745 -0.684563  ...  0.148944 -0.559822  0.031149 -0.434878   \n",
       "\n",
       "         363       364       365       366       367       368  \n",
       "0   1.131037 -0.890519  1.529835 -1.232949  0.090685  0.055691  \n",
       "1   1.600488  0.044756 -0.760092 -0.238756 -0.299987  0.111686  \n",
       "2  -0.300688 -2.007196  0.283148 -0.248308 -0.229540 -0.453543  \n",
       "3   0.683187 -0.927865  0.805871 -1.548329 -0.953638  0.863869  \n",
       "4   0.189521 -0.826415  0.103036  0.392769 -0.268875  0.310314  \n",
       "5   0.583457  0.935904  0.564009  0.285819 -1.098610  0.814702  \n",
       "6   1.095927  0.113217  0.810632  0.651877 -0.531707  0.015332  \n",
       "7   0.036079  0.184241  0.705365 -1.548329 -0.377966  0.792129  \n",
       "8  -0.354667 -1.177600  0.659029  1.004194  0.403447  1.367558  \n",
       "9  -0.082349  0.292043  0.190434  2.515544  0.575613  0.644237  \n",
       "10 -0.006238  1.063312  0.099768 -1.214778  0.552570 -0.941951  \n",
       "11  0.690180 -0.248975  0.475366 -0.946281  0.196263 -0.082787  \n",
       "12 -0.738260  1.102037  0.677370  0.585452  0.117570  1.788051  \n",
       "13 -0.555926 -1.163196 -2.158674  1.046067 -0.340963 -2.843124  \n",
       "14 -2.635975  1.109899  0.889646  2.381625  0.573255  1.313238  \n",
       "15  1.379386 -0.109616  0.546066 -0.924909 -1.224564  0.073066  \n",
       "16 -0.421575 -1.746287  1.480470 -0.866919  0.979884  0.559317  \n",
       "17 -2.294624  1.196131 -2.372160 -0.999612  0.212737 -0.012930  \n",
       "18  0.652486  0.031301 -0.463545 -1.058741 -0.341246  0.644786  \n",
       "19  0.144553 -0.068699 -0.169576  1.626570  2.237907  0.210993  \n",
       "20 -0.253161  0.199152  0.026295 -0.822217  1.425170 -0.012027  \n",
       "21  0.520552  1.142800 -1.983380 -0.450623  0.759296  0.839734  \n",
       "22 -0.142327 -1.908463  0.693201 -0.325211 -0.887135 -1.654832  \n",
       "23 -1.437294  0.864964 -1.153901  1.079469 -0.241707  0.385623  \n",
       "24 -1.285238 -0.067270 -0.897504  2.443284 -0.171860 -2.278283  \n",
       "25 -1.205443  0.978404 -0.224551 -0.273562  0.486103 -1.848926  \n",
       "26 -0.726191  1.107486 -0.315785 -1.029691 -0.137388  1.385896  \n",
       "27  1.303926  0.995660 -2.144002 -1.548329 -0.172931 -0.697276  \n",
       "28  0.198070 -0.750083 -0.084595  0.410953  1.618637 -0.110634  \n",
       "29  0.611556  0.736876  1.531226 -1.548329 -0.239290  0.768947  \n",
       "30  1.620704 -0.283523  0.837234 -0.450286 -1.793893  0.026374  \n",
       "31 -0.185562  0.930015  0.012071 -1.076205 -0.673049 -0.319628  \n",
       "32 -2.109601  1.261469  2.586153 -0.382131  1.362311  0.768245  \n",
       "33 -0.327523  0.866535 -0.611067  0.709982  1.282063 -3.303747  \n",
       "34  0.844525  0.036370  0.446342  0.314849  0.271055 -0.532813  \n",
       "35 -1.768187  1.245896 -0.927493 -0.640630  1.812766  0.718375  \n",
       "36 -1.396645  0.155956 -0.703042  0.760250  1.297756 -1.114401  \n",
       "37 -1.409975  0.958414 -1.225443  0.251200 -0.759197  0.504428  \n",
       "38  0.303727 -0.534743 -0.060109 -0.170359 -2.022562 -0.301322  \n",
       "39  0.466996  0.792390 -1.567516  1.905848  0.531075  0.928946  \n",
       "40  0.850215 -0.071967 -0.941385 -1.130990 -1.309602  0.059633  \n",
       "41 -0.044227 -2.218069  0.437456  1.170048  0.450166  0.980280  \n",
       "42  0.730341 -1.753375  1.135299  0.798803  1.164539  0.410870  \n",
       "43  0.711725 -0.071517  0.211145 -0.074202  1.016763 -1.091769  \n",
       "44  0.654962  0.768243  0.087504  0.462239  0.119469  0.574085  \n",
       "45  1.115714  0.521213 -1.115568  0.741069 -0.331798  0.229410  \n",
       "46  0.464914  0.689402  0.064300 -0.373452  0.230564  1.122755  \n",
       "47  0.958600 -0.018619 -0.282975 -0.278644 -0.356848 -0.154458  \n",
       "48  1.552780  0.746527 -0.798385  0.394120 -0.326934  0.924987  \n",
       "49  1.357958  0.098578 -0.070107 -0.340895 -2.261890  0.261115  \n",
       "50 -1.155087  0.386314 -0.286913 -1.110409  0.372993 -0.036534  \n",
       "51 -0.721036  0.682624  1.126328  0.479468 -1.442462  0.278994  \n",
       "52 -0.605020 -2.766362 -0.710676  0.480753  1.941014 -2.107872  \n",
       "53  0.682308 -1.642583  0.262188 -0.403934  1.294226  0.156651  \n",
       "54  0.672375  0.020585  0.538960  0.527446 -1.808974 -0.407521  \n",
       "55 -1.237360  0.153206  0.197136  0.763262 -0.491389 -0.816772  \n",
       "56  0.548657  0.089122 -0.084421 -0.727874  0.522598 -0.109135  \n",
       "57  0.080675 -1.411915 -0.578436 -0.367693 -1.643935  0.280959  \n",
       "58 -0.487742 -0.946363  0.246197  0.691111 -0.435208 -0.303709  \n",
       "59 -0.549658  1.110178  2.432223 -0.520495 -0.723350  0.364719  \n",
       "\n",
       "[60 rows x 369 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_rf = np.delete(X, obj=~np.array(rf_sel_features), axis=1)\n",
    "pd.DataFrame(X_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OverSampling Analysis Using ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 866091)\n",
      "(60,)\n",
      "After OverSampling\n",
      "(90, 866091)\n",
      "(90,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(cpg_sites)\n",
    "Y = labels\n",
    "#print(X)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print (\"After OverSampling\")\n",
    "oversampling = SMOTE()\n",
    "X,Y = oversampling.fit_resample(X,Y)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features: 866091\n",
      "85% of total feature 736177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [475093] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features using ANOVA F-test: 134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "print(\"Total number of features:\", X.shape[1])\n",
    "a = round(0.85*(X.shape[1]))\n",
    "print(\"85% of total feature\", a)\n",
    "X_new = SelectKBest(f_classif, k=a).fit_transform(X, Y)\n",
    "#df=pd.DataFrame(X_new)\n",
    "fvals, pvals = f_classif(X_new, Y)\n",
    "to_remove = pvals >= (0.05/X_new.shape[1])\n",
    "X_anova = np.delete(X_new, obj=to_remove, axis=1)\n",
    "print(\"Number of selected features using ANOVA F-test:\", X_anova.shape[1])\n",
    "#print(X_anova)\n",
    "\n",
    "df1= cpg_sites_oversample\n",
    "print(\"Features using ANOVA\")\n",
    "bestfeatures = SelectKBest(score_func=f_classif, k=X_anova.shape[1])\n",
    "#df1 = pd.DataFrame(X)\n",
    "anova = bestfeatures.fit(df1, Y)\n",
    "dfscores = pd.DataFrame(anova.scores_)\n",
    "dfcolumns = pd.DataFrame(df1.columns)\n",
    "# concat two dataframes for better visualization\n",
    "featureScores = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "featureScores.columns = ['fFeature', 'fScore']  # naming the dataframe columns\n",
    "best_features = featureScores.nlargest(X_anova.shape[1], 'fScore')\n",
    "#print(best_features)\n",
    "anova_bestfeatures = best_features['fFeature'].tolist()\n",
    "print(len(anova_bestfeatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cpg_sites_oversample[anova_bestfeatures]\n",
    "y = Y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "# X_train_standardized = scaler.fit_transform(X_train)\n",
    "# X_test_standardized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = \"SGD Classifier\"\n",
    "Feature = \"CV + ENET\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.1)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#  For loop using GRID CV (for cross validation)\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [50, 100, 200],\n",
    "    # 'max_depth': [None, 10, 20],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# rf_classifier = RandomForestClassifier()\n",
    "# \n",
    "# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "# \n",
    "#  For Loop in Models\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [i for i in range(1,50,5)],\n",
    "    # 'max_depth': [None, 10],\n",
    "    # 'min_samples_split': [2, 5],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': [ 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1],\n",
    "# }\n",
    "# best_accuracy = 0.0\n",
    "# best_params = None\n",
    "# import itertools\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "    # param_dict = dict(zip(param_grid.keys(), params))\n",
    "# \n",
    "    # rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "    # rf_classifier.fit(X_train, y_train)\n",
    "# \n",
    "    # y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "    # val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "# \n",
    "    # if val_accuracy > best_accuracy:\n",
    "        # best_accuracy = val_accuracy\n",
    "        # best_params = param_dict\n",
    "# \n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGD Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': [None,1,2,3,4,5, 10],\n",
    "#     'min_samples_split': [2, 3, 4, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4,5],\n",
    "#     'max_features': [None, 'sqrt', 'log2']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.8333333333333334\n",
      "Lancent :  0.6666666666666666\n",
      "Melanoma :  0.676923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier(**best_params, random_state=20)\n",
    "model.fit(X_train,y_train)\n",
    "print(\"Train : \" ,model.score(X_test,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100],\n",
    "#     'learning_rate': [0.01, 0.1, 0.2],\n",
    "#     'max_depth': [3, 5],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'subsample': [0.8, 0.9, 1.0],\n",
    "#     'max_features': ['sqrt', 'log2', None]\n",
    "# }\n",
    "# best_accuracy = 0.0\n",
    "# best_params = None\n",
    "# import itertools\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "#     param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "#     rf_classifier = GradientBoostingClassifier(random_state=20, **param_dict)\n",
    "#     rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "#     y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "#     val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    \n",
    "#     # val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "#     # val_accuracy = accuracy_score(lancent_y, val2_pred)\n",
    "\n",
    "#     # val_accuracy = (val_accuracy + val2_accuracy) / 2\n",
    "#     if val_accuracy > best_accuracy:\n",
    "#         best_accuracy = val_accuracy\n",
    "#         best_params = param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3993 - mean_absolute_error: 0.1774 - accuracy: 0.8333\n",
      "Test : [0.3992656171321869, 0.17739303410053253, 0.8333333134651184]\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.3871 - mean_absolute_error: 0.4425 - accuracy: 0.5926\n",
      "Lancent : [1.3871243000030518, 0.4424840807914734, 0.5925925970077515]\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 1.1508 - mean_absolute_error: 0.4644 - accuracy: 0.5538\n",
      "Melanome : [1.1508457660675049, 0.4643641412258148, 0.5538461804389954]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "NN_model = Sequential()\n",
    "NN_model = Sequential()\n",
    "# Input layer\n",
    "NN_model.add(Dense(128, kernel_initializer='normal', input_dim=X_train.shape[1], activation='relu'))\n",
    "NN_model.add(Dropout(0.5))\n",
    "# Hidden layers\n",
    "NN_model.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
    "NN_model.add(Dropout(0.8))\n",
    "\n",
    "NN_model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "NN_model.add(Dropout(0.9))\n",
    "\n",
    "# Output layer\n",
    "NN_model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "# Compile NN\n",
    "NN_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01), metrics=['mean_absolute_error', 'accuracy'])\n",
    "# NN_model.summary()\n",
    "\n",
    "history = NN_model.fit(X_train_standardized,y_train, verbose=0, batch_size=64, validation_split=0.1 ,epochs=20)\n",
    "# pd.DataFrame(history.history).tail()\n",
    "print(\"Test :\" ,NN_model.evaluate(X_test_standardized,y_test))\n",
    "print(\"Lancent :\", NN_model.evaluate(lancent_test_standarized,lancent_y))\n",
    "print('Melanome :', NN_model.evaluate(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 61ms/step\n",
      "MLP Model Test Accuracy: 0.8333333333333334\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "MLP Model Lancent Accuracy: 0.6790123456790124\n",
      "3/3 [==============================] - 0s 2ms/step\n",
      "MLP Model Melanoma Accuracy: 0.6615384615384615\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "# Build the MLP model using Keras\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Dense(units=30, input_dim=X_train.shape[1], activation='linear'))\n",
    "\n",
    "model.add(Dropout(0.8))\n",
    "# Add hidden layers\n",
    "model.add(Dense(units=60, activation='relu'))\n",
    "\n",
    "# Add output layer (assuming binary classification)\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, verbose=0)\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"MLP Model Test Accuracy:\", accuracy)\n",
    "print(\"MLP Model Lancent Accuracy:\", accuracy_score(model.predict(((lancent_test_standarized)>0.5)).astype(int),lancent_y))\n",
    "print(\"MLP Model Melanoma Accuracy:\", accuracy_score(model.predict(((melanoma_test_standarized)>0.5)).astype(int),melanoma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" checked><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 1500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF = RandomForestClassifier()\n",
    "RF.fit(X,Y)\n",
    "dfscores = pd.DataFrame(RF.feature_importances_)\n",
    "dfcolumns = pd.DataFrame(cpg_sites.columns)\n",
    "fs = pd.concat([dfcolumns, dfscores], axis=1)\n",
    "fs.columns = ['rfFeature', 'rfScore']\n",
    "rf_cpgs = fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False)['rfFeature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2096,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X)\n",
    "X_df.columns = cpg_sites.columns\n",
    "X_df = X_df[rf_cpgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(min_samples_leaf=4, n_estimators=7, n_jobs=-1,\n",
       "                       random_state=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Saved MOdel \n",
    "import joblib\n",
    "new_model = joblib.load('Saving Models/RF 70% on both.joblib')\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2097,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  0.8333333333333334\n",
      "Lancent :  0.7283950617283951\n",
      "Melanoma :  0.7076923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = new_model# RandomForestClassifier(**best_params, random_state=0)\n",
    "model.fit(X_train.values,y_train)\n",
    "print(\"Train : \" ,model.score(X_test,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2085,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [i for i in range(1,30)],\n",
    "    'max_depth': [None,2,3,5,8],\n",
    "    'min_samples_split': [2,3,4,5,6],\n",
    "    'min_samples_leaf': [ 1,2,3,4],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'n_jobs' : [-1],\n",
    "}\n",
    "params_list = []\n",
    "best_accuracy = 0.0\n",
    "best_params = None\n",
    "import itertools\n",
    "for params in itertools.product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "    rf_classifier.fit(X_train.values, y_train)\n",
    "\n",
    "    y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "    val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    \n",
    "    val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "    val2_accuracy = accuracy_score(lancent_y, val2_pred)\n",
    "    data_dict = {}\n",
    "    data_dict['Lancent'] = val2_accuracy\n",
    "    data_dict['Melanoma'] = val_accuracy\n",
    "    data_dict['Params'] = param_dict\n",
    "    params_list.append(data_dict)\n",
    "    # val_accuracy = (val_accuracy + val2_accuracy) / 2\n",
    "    if val_accuracy >= 0.7 and val2_accuracy >= 0.7:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_params = param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2110,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False).to_csv('RANDOM FOREST OVERSAMPLED CPGs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2111,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_df = fs[fs['rfScore'] > 0].sort_values(by='rfScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfFeature</th>\n",
       "      <th>rfScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>cg00031256</td>\n",
       "      <td>0.000416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>cg00035847</td>\n",
       "      <td>0.001243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>cg00049220</td>\n",
       "      <td>0.001125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2401</th>\n",
       "      <td>cg00064250</td>\n",
       "      <td>0.002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>cg00232731</td>\n",
       "      <td>0.003968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852571</th>\n",
       "      <td>cg27307781</td>\n",
       "      <td>0.001817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858795</th>\n",
       "      <td>cg27518051</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859820</th>\n",
       "      <td>cg27552081</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861169</th>\n",
       "      <td>cg27598086</td>\n",
       "      <td>0.000414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865431</th>\n",
       "      <td>ch.6.12702243F</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>429 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rfFeature   rfScore\n",
       "1193        cg00031256  0.000416\n",
       "1365        cg00035847  0.001243\n",
       "1858        cg00049220  0.001125\n",
       "2401        cg00064250  0.002363\n",
       "8302        cg00232731  0.003968\n",
       "...                ...       ...\n",
       "852571      cg27307781  0.001817\n",
       "858795      cg27518051  0.001947\n",
       "859820      cg27552081  0.000741\n",
       "861169      cg27598086  0.000414\n",
       "865431  ch.6.12702243F  0.002488\n",
       "\n",
       "[429 rows x 2 columns]"
      ]
     },
     "execution_count": 2113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_df.sort_values(by='rfFeature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2093,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lancent</th>\n",
       "      <th>Melanoma</th>\n",
       "      <th>Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': None, 'min_sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>{'n_estimators': 6, 'max_depth': 5, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 5, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.530864</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>{'n_estimators': 3, 'max_depth': 8, 'min_sampl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1680 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Lancent  Melanoma                                             Params\n",
       "624  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "625  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "632  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "633  0.703704  0.569231  {'n_estimators': 6, 'max_depth': None, 'min_sa...\n",
       "664  0.703704  0.569231  {'n_estimators': 6, 'max_depth': 5, 'min_sampl...\n",
       "..        ...       ...                                                ...\n",
       "315  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 5, 'min_sampl...\n",
       "346  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "347  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "354  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "355  0.530864  0.615385  {'n_estimators': 3, 'max_depth': 8, 'min_sampl...\n",
       "\n",
       "[1680 rows x 3 columns]"
      ]
     },
     "execution_count": 2093,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.sort_values(by=['Lancent',\"Melanoma\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1868,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['CPGs'] = rf_cpgs\n",
    "df['Score'] = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1874,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_extra_CPGS = df[df['Score'] != 0]['CPGs'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1880,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rf = X_df[rf_extra_CPGS]\n",
    "Y_rf = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1881,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_rf, Y_rf, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ABCMeta object argument after ** must be a mapping, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sbl/Documents/Cancer Methylation/new analysis/new.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m RandomForestClassifier(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbest_params, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train\u001b[39m.\u001b[39mvalues,y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y144sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain : \u001b[39m\u001b[39m\"\u001b[39m ,model\u001b[39m.\u001b[39mscore(X_test\u001b[39m.\u001b[39mvalues,y_test))\n",
      "\u001b[0;31mTypeError\u001b[0m: ABCMeta object argument after ** must be a mapping, not NoneType"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(**best_params, random_state=42)\n",
    "model.fit(X_train.values,y_train)\n",
    "print(\"Train : \" ,model.score(X_test.values,y_test))\n",
    "print(\"Lancent : \",model.score(lancent_test_standarized,lancent_y))\n",
    "print(\"Melanoma : \" ,model.score(melanoma_test_standarized,melanoma_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1828,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for params in itertools.product(*param_grid.values()):\n",
    "    param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=0, **param_dict)\n",
    "    rf_classifier.fit(X_train.values, y_train)\n",
    "\n",
    "    y_val_pred1 = rf_classifier.predict(melanoma_test_standarized)\n",
    "    val1_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "    y_val2_pred = rf_classifier.predict(lancent_test_standarized)\n",
    "    val2_accuracy = accuracy_score(lancent_y, y_val2_pred)\n",
    "    print(val1_accuracy,val2_accuracy)\n",
    "#   val1_accuracy = 60\n",
    "#   val2_accuracy = 70\n",
    "    if val1_accuracy > 0.7 and val2_accuracy > 0.7:\n",
    "        data_dict = {}\n",
    "        data_dict['Lancent'] = val2_accuracy\n",
    "        data_dict['Melanoma'] =  val1_accuracy\n",
    "        data_dict['Params'] = param_dict\n",
    "        params_list.append(data_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1809,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = joblib.load('./Saving Models/RF 70% on both.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1836,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'float' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sbl/Documents/Cancer Methylation/new analysis/new.ipynb Cell 35\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m val1_accuracy \u001b[39m=\u001b[39m \u001b[39m0.6615384615384615\u001b[39m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m val2_accuracy \u001b[39m=\u001b[39m \u001b[39m0.5555555555555556\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mif\u001b[39;00m val1_accuracy \u001b[39m&\u001b[39;49m val2_accuracy \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m70\u001b[39m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     data_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sbl/Documents/Cancer%20Methylation/new%20analysis/new.ipynb#Y135sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data_dict[\u001b[39m'\u001b[39m\u001b[39mLancent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m val1_accuracy\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'float' and 'float'"
     ]
    }
   ],
   "source": [
    "params_list = []\n",
    "val1_accuracy = 0.6615384615384615 \n",
    "val2_accuracy = 0.5555555555555556\n",
    "if val1_accuracy & val2_accuracy >= 70:\n",
    "    data_dict = {}\n",
    "    data_dict['Lancent'] = val1_accuracy\n",
    "    data_dict['Melanoma'] =  val2_accuracy\n",
    "    data_dict['Params'] = param_dict\n",
    "    params_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'No of Labels before OverSampling (original)'}, xlabel='labels'>"
      ]
     },
     "execution_count": 2169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = pd.DataFrame()\n",
    "label['labels'] = labels #.value_counts().plot(kind ='bar')\n",
    "label['labels'].astype(str).replace('1',\"Responsive\").replace('0','Non Responsive').value_counts().plot(kind='bar', color='darkorange', title=\"No of Labels before OverSampling (original)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f2a4b311610>"
      ]
     },
     "execution_count": 2181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.scatter(cpg_sites.mean(axis=1), cpg_sites.median(axis=1), c=la)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbl/.local/lib/python3.8/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n",
      "  self._figure.tight_layout(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 558.875x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.lmplot(x= 'Labels', y= 'cg00000029',data= train, hue='Labels', fit_reg=False)\n",
    "fig = plt.gcf()\n",
    "# set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg20941745</th>\n",
       "      <th>cg11575131</th>\n",
       "      <th>cg01229787</th>\n",
       "      <th>cg20029669</th>\n",
       "      <th>cg10363003</th>\n",
       "      <th>cg13606420</th>\n",
       "      <th>cg22134634</th>\n",
       "      <th>cg18493677</th>\n",
       "      <th>cg24083367</th>\n",
       "      <th>cg22074576</th>\n",
       "      <th>...</th>\n",
       "      <th>cg16429735</th>\n",
       "      <th>cg17773637</th>\n",
       "      <th>cg24016627</th>\n",
       "      <th>cg11309112</th>\n",
       "      <th>cg24145481</th>\n",
       "      <th>cg17956443</th>\n",
       "      <th>cg23288725</th>\n",
       "      <th>cg25846864</th>\n",
       "      <th>cg01327313</th>\n",
       "      <th>cg05329576</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.780868</td>\n",
       "      <td>0.469294</td>\n",
       "      <td>0.635856</td>\n",
       "      <td>0.811198</td>\n",
       "      <td>-0.024755</td>\n",
       "      <td>4.689193</td>\n",
       "      <td>-0.543691</td>\n",
       "      <td>1.036917</td>\n",
       "      <td>3.485788</td>\n",
       "      <td>0.054967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.962014</td>\n",
       "      <td>0.544353</td>\n",
       "      <td>0.158179</td>\n",
       "      <td>0.022410</td>\n",
       "      <td>0.366729</td>\n",
       "      <td>0.456338</td>\n",
       "      <td>1.201027</td>\n",
       "      <td>-2.212670</td>\n",
       "      <td>0.197558</td>\n",
       "      <td>-0.357045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.335933</td>\n",
       "      <td>1.780301</td>\n",
       "      <td>1.287080</td>\n",
       "      <td>-0.179507</td>\n",
       "      <td>-0.747773</td>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.204768</td>\n",
       "      <td>1.028305</td>\n",
       "      <td>2.193005</td>\n",
       "      <td>-0.798825</td>\n",
       "      <td>...</td>\n",
       "      <td>1.094258</td>\n",
       "      <td>1.176522</td>\n",
       "      <td>-0.168442</td>\n",
       "      <td>-0.201650</td>\n",
       "      <td>1.544387</td>\n",
       "      <td>0.648869</td>\n",
       "      <td>1.240905</td>\n",
       "      <td>0.320675</td>\n",
       "      <td>-0.019328</td>\n",
       "      <td>-0.276183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.677504</td>\n",
       "      <td>1.185740</td>\n",
       "      <td>0.226599</td>\n",
       "      <td>0.148077</td>\n",
       "      <td>-0.279278</td>\n",
       "      <td>-0.329107</td>\n",
       "      <td>0.865019</td>\n",
       "      <td>0.591379</td>\n",
       "      <td>1.116660</td>\n",
       "      <td>-0.306906</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027010</td>\n",
       "      <td>-0.083404</td>\n",
       "      <td>-0.229269</td>\n",
       "      <td>0.836217</td>\n",
       "      <td>0.490400</td>\n",
       "      <td>0.335833</td>\n",
       "      <td>0.780244</td>\n",
       "      <td>0.982946</td>\n",
       "      <td>0.225118</td>\n",
       "      <td>0.020902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.522022</td>\n",
       "      <td>-1.387540</td>\n",
       "      <td>1.141564</td>\n",
       "      <td>-0.860836</td>\n",
       "      <td>0.651938</td>\n",
       "      <td>-0.867817</td>\n",
       "      <td>-1.054429</td>\n",
       "      <td>0.666018</td>\n",
       "      <td>-0.930475</td>\n",
       "      <td>-0.890086</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.901296</td>\n",
       "      <td>0.228058</td>\n",
       "      <td>-0.100805</td>\n",
       "      <td>-1.197182</td>\n",
       "      <td>0.666047</td>\n",
       "      <td>0.575147</td>\n",
       "      <td>1.156405</td>\n",
       "      <td>0.358052</td>\n",
       "      <td>0.218179</td>\n",
       "      <td>-0.392412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.030379</td>\n",
       "      <td>1.156289</td>\n",
       "      <td>0.180130</td>\n",
       "      <td>-0.148074</td>\n",
       "      <td>2.007100</td>\n",
       "      <td>0.469516</td>\n",
       "      <td>1.024705</td>\n",
       "      <td>0.224214</td>\n",
       "      <td>2.059828</td>\n",
       "      <td>-0.835504</td>\n",
       "      <td>...</td>\n",
       "      <td>1.441614</td>\n",
       "      <td>-0.216614</td>\n",
       "      <td>-0.064767</td>\n",
       "      <td>-0.113365</td>\n",
       "      <td>0.623185</td>\n",
       "      <td>0.688637</td>\n",
       "      <td>0.517251</td>\n",
       "      <td>0.712552</td>\n",
       "      <td>-0.460893</td>\n",
       "      <td>-0.856107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>-2.087876</td>\n",
       "      <td>0.735790</td>\n",
       "      <td>0.737643</td>\n",
       "      <td>1.025053</td>\n",
       "      <td>0.253717</td>\n",
       "      <td>1.526867</td>\n",
       "      <td>-0.250301</td>\n",
       "      <td>0.466442</td>\n",
       "      <td>0.978328</td>\n",
       "      <td>0.721022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.451027</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>-0.144130</td>\n",
       "      <td>-0.721000</td>\n",
       "      <td>-0.094146</td>\n",
       "      <td>0.518878</td>\n",
       "      <td>-0.591792</td>\n",
       "      <td>-0.714019</td>\n",
       "      <td>-1.936203</td>\n",
       "      <td>-0.579798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-0.487292</td>\n",
       "      <td>1.302481</td>\n",
       "      <td>1.364562</td>\n",
       "      <td>0.675667</td>\n",
       "      <td>0.980441</td>\n",
       "      <td>1.267139</td>\n",
       "      <td>-0.445336</td>\n",
       "      <td>0.797090</td>\n",
       "      <td>0.961974</td>\n",
       "      <td>0.427999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499667</td>\n",
       "      <td>0.496132</td>\n",
       "      <td>-0.112267</td>\n",
       "      <td>-0.080733</td>\n",
       "      <td>1.012053</td>\n",
       "      <td>0.375857</td>\n",
       "      <td>0.676184</td>\n",
       "      <td>0.181325</td>\n",
       "      <td>-0.320996</td>\n",
       "      <td>0.481265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>-1.188083</td>\n",
       "      <td>0.812195</td>\n",
       "      <td>1.309263</td>\n",
       "      <td>2.873465</td>\n",
       "      <td>0.471076</td>\n",
       "      <td>0.952921</td>\n",
       "      <td>0.078194</td>\n",
       "      <td>0.700741</td>\n",
       "      <td>1.046980</td>\n",
       "      <td>1.194344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.213552</td>\n",
       "      <td>0.328981</td>\n",
       "      <td>-0.143044</td>\n",
       "      <td>-0.071150</td>\n",
       "      <td>-0.210137</td>\n",
       "      <td>0.842936</td>\n",
       "      <td>-0.393622</td>\n",
       "      <td>0.215271</td>\n",
       "      <td>-1.185571</td>\n",
       "      <td>0.018176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.112609</td>\n",
       "      <td>0.183431</td>\n",
       "      <td>0.300688</td>\n",
       "      <td>0.242444</td>\n",
       "      <td>0.566381</td>\n",
       "      <td>0.927712</td>\n",
       "      <td>-0.651305</td>\n",
       "      <td>0.381057</td>\n",
       "      <td>1.178076</td>\n",
       "      <td>0.978378</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.145917</td>\n",
       "      <td>0.237814</td>\n",
       "      <td>-0.171871</td>\n",
       "      <td>-0.910843</td>\n",
       "      <td>-0.381000</td>\n",
       "      <td>0.452552</td>\n",
       "      <td>0.414618</td>\n",
       "      <td>-0.716805</td>\n",
       "      <td>0.329390</td>\n",
       "      <td>-0.697732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.155072</td>\n",
       "      <td>0.493463</td>\n",
       "      <td>0.288611</td>\n",
       "      <td>0.476221</td>\n",
       "      <td>0.420652</td>\n",
       "      <td>1.137031</td>\n",
       "      <td>-1.234340</td>\n",
       "      <td>0.475065</td>\n",
       "      <td>1.442044</td>\n",
       "      <td>0.428660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.916414</td>\n",
       "      <td>0.116135</td>\n",
       "      <td>-0.171963</td>\n",
       "      <td>-0.678457</td>\n",
       "      <td>-0.147978</td>\n",
       "      <td>0.552619</td>\n",
       "      <td>0.527432</td>\n",
       "      <td>-0.398744</td>\n",
       "      <td>0.139158</td>\n",
       "      <td>-0.101257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 429 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cg20941745  cg11575131  cg01229787  cg20029669  cg10363003  cg13606420  \\\n",
       "0     0.780868    0.469294    0.635856    0.811198   -0.024755    4.689193   \n",
       "1     0.335933    1.780301    1.287080   -0.179507   -0.747773    0.167730   \n",
       "2     0.677504    1.185740    0.226599    0.148077   -0.279278   -0.329107   \n",
       "3    -0.522022   -1.387540    1.141564   -0.860836    0.651938   -0.867817   \n",
       "4     0.030379    1.156289    0.180130   -0.148074    2.007100    0.469516   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "85   -2.087876    0.735790    0.737643    1.025053    0.253717    1.526867   \n",
       "86   -0.487292    1.302481    1.364562    0.675667    0.980441    1.267139   \n",
       "87   -1.188083    0.812195    1.309263    2.873465    0.471076    0.952921   \n",
       "88    0.112609    0.183431    0.300688    0.242444    0.566381    0.927712   \n",
       "89    0.155072    0.493463    0.288611    0.476221    0.420652    1.137031   \n",
       "\n",
       "    cg22134634  cg18493677  cg24083367  cg22074576  ...  cg16429735  \\\n",
       "0    -0.543691    1.036917    3.485788    0.054967  ...   -0.962014   \n",
       "1     0.204768    1.028305    2.193005   -0.798825  ...    1.094258   \n",
       "2     0.865019    0.591379    1.116660   -0.306906  ...   -0.027010   \n",
       "3    -1.054429    0.666018   -0.930475   -0.890086  ...   -0.901296   \n",
       "4     1.024705    0.224214    2.059828   -0.835504  ...    1.441614   \n",
       "..         ...         ...         ...         ...  ...         ...   \n",
       "85   -0.250301    0.466442    0.978328    0.721022  ...    0.451027   \n",
       "86   -0.445336    0.797090    0.961974    0.427999  ...    0.499667   \n",
       "87    0.078194    0.700741    1.046980    1.194344  ...   -0.213552   \n",
       "88   -0.651305    0.381057    1.178076    0.978378  ...   -1.145917   \n",
       "89   -1.234340    0.475065    1.442044    0.428660  ...   -0.916414   \n",
       "\n",
       "    cg17773637  cg24016627  cg11309112  cg24145481  cg17956443  cg23288725  \\\n",
       "0     0.544353    0.158179    0.022410    0.366729    0.456338    1.201027   \n",
       "1     1.176522   -0.168442   -0.201650    1.544387    0.648869    1.240905   \n",
       "2    -0.083404   -0.229269    0.836217    0.490400    0.335833    0.780244   \n",
       "3     0.228058   -0.100805   -1.197182    0.666047    0.575147    1.156405   \n",
       "4    -0.216614   -0.064767   -0.113365    0.623185    0.688637    0.517251   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "85    0.032645   -0.144130   -0.721000   -0.094146    0.518878   -0.591792   \n",
       "86    0.496132   -0.112267   -0.080733    1.012053    0.375857    0.676184   \n",
       "87    0.328981   -0.143044   -0.071150   -0.210137    0.842936   -0.393622   \n",
       "88    0.237814   -0.171871   -0.910843   -0.381000    0.452552    0.414618   \n",
       "89    0.116135   -0.171963   -0.678457   -0.147978    0.552619    0.527432   \n",
       "\n",
       "    cg25846864  cg01327313  cg05329576  \n",
       "0    -2.212670    0.197558   -0.357045  \n",
       "1     0.320675   -0.019328   -0.276183  \n",
       "2     0.982946    0.225118    0.020902  \n",
       "3     0.358052    0.218179   -0.392412  \n",
       "4     0.712552   -0.460893   -0.856107  \n",
       "..         ...         ...         ...  \n",
       "85   -0.714019   -1.936203   -0.579798  \n",
       "86    0.181325   -0.320996    0.481265  \n",
       "87    0.215271   -1.185571    0.018176  \n",
       "88   -0.716805    0.329390   -0.697732  \n",
       "89   -0.398744    0.139158   -0.101257  \n",
       "\n",
       "[90 rows x 429 columns]"
      ]
     },
     "execution_count": 1520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model = \"SGD Classifier\"\n",
    "Feature = \"CV + ENET\"\n",
    "def print_metrics(x, y):\n",
    "    print(f\" --------  {Model} {Feature} Accuracy:\", accuracy_score(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Confusion Matrix:\\n\", confusion_matrix(x, y))\n",
    "    print(f\"\\n{Model} {Feature} Classification Report:\\n\", classification_report(x, y))\n",
    "\n",
    "\n",
    "# # Linear Regression \n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train_standardized, y_train)\n",
    "# y_pred = linear_model.predict(X_test_standardized)\n",
    "# classification_report(y_pred, y_test)\n",
    "# # Output of LINEAR REGRESSION :: Can not map binary values with floats\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                               -------------------------------- LOGISTIC REGRESSION ANALYSIS\n",
    "\n",
    "# model = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.1)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                             \n",
    "#                                             ------------------ Random Forest Analysis \n",
    "\n",
    "# model = RandomForestClassifier(n_estimators = 100, random_state = 220)\n",
    "# model = RandomForestClassifier(n_estimators=150 ,max_features= 5 , random_state=20)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#  For loop using GRID CV (for cross validation)\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [50, 100, 200],\n",
    "    # 'max_depth': [None, 10, 20],\n",
    "    # 'min_samples_split': [2, 5, 10],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1]\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# rf_classifier = RandomForestClassifier()\n",
    "# \n",
    "# grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy')\n",
    "# grid_search.fit(X_train, y_train)\n",
    "# \n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "# \n",
    "#  For Loop in Models\n",
    "# param_grid = {\n",
    "    # 'n_estimators': [i for i in range(1,50,5)],\n",
    "    # 'max_depth': [None, 10],\n",
    "    # 'min_samples_split': [2, 5],\n",
    "    # 'min_samples_leaf': [1, 2, 4],\n",
    "    # 'max_features': [ 'sqrt', 'log2'],\n",
    "    # 'n_jobs' : [-1],\n",
    "# }\n",
    "# best_accuracy = 0.0\n",
    "# best_params = None\n",
    "# import itertools\n",
    "# for params in itertools.product(*param_grid.values()):\n",
    "    # param_dict = dict(zip(param_grid.keys(), params))\n",
    "# \n",
    "    # rf_classifier = RandomForestClassifier(random_state=42, **param_dict)\n",
    "    # rf_classifier.fit(X_train, y_train)\n",
    "# \n",
    "    # y_val_pred = rf_classifier.predict(melanoma_test_standarized)\n",
    "    # val_accuracy = accuracy_score(melanoma_y, y_val_pred)\n",
    "# \n",
    "    # if val_accuracy > best_accuracy:\n",
    "        # best_accuracy = val_accuracy\n",
    "        # best_params = param_dict\n",
    "# \n",
    "\n",
    "#                                                             \n",
    "#                                         -------------------------- SGD Classifier\n",
    "\n",
    "# model = SGDClassifier(max_iter=500, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "#                                                             \n",
    "#                                                --------------------- SVM Classifier \n",
    "\n",
    "# model = LinearSVC(dual=\"auto\", random_state=0, tol=1e-3)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                               ------------------------- Gradient Boost\n",
    "\n",
    "# params = {'n_estimators':2, 'max_depth':1, 'learning_rate': 0.4}\n",
    "# model = GradientBoostingClassifier(**params)\n",
    "# model.fit(X_train_standardized, y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "#                                                               ------------------- HistGradientBoostingClassifier\n",
    "\n",
    "# model = HistGradientBoostingClassifier(min_samples_leaf= 1 ,max_depth=2, learning_rate=0.5,max_iter=200)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# y_pred = model.predict(X_test_standardized)\n",
    "# print_metrics(y_pred,y_test)\n",
    "# y_pred = model.predict(lancent_test_standarized)\n",
    "# print_metrics(y_pred,lancent_y)\n",
    "# y_pred = model.predict(melanoma_test_standarized)\n",
    "# print_metrics(y_pred,melanoma_y)\n",
    "\n",
    "\n",
    "\n",
    "# # model = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "# model.fit(X_train_standardized,y_train)\n",
    "# print(\"Test :\", model.score(X_test_standardized,y_test))\n",
    "# print(\"Lancent :\", model.score(lancent_test_standarized,lancent_y))\n",
    "# print(\"Melanoma :\", model.score(melanoma_test_standarized,melanoma_y))\n",
    "# param_grid = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'splitter': ['best', 'random'],\n",
    "#     'max_depth': [None,1,2,3,4,5, 10],\n",
    "#     'min_samples_split': [2, 3, 4, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4,5],\n",
    "#     'max_features': [None, 'sqrt', 'log2']\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
